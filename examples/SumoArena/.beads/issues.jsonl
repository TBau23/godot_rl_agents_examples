{"id":"SumoArena-09y","title":"Nerf swing push to 2x multiplier","description":"","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-06T21:53:49.891358-06:00","updated_at":"2025-12-06T21:56:10.709084-06:00","closed_at":"2025-12-06T21:56:10.709084-06:00"}
{"id":"SumoArena-0v1","title":"Replace capsules with sumo blob models","description":"# Replace Capsules with Sumo Blob Models\n\n## What\nReplace the debug capsule meshes with low-poly sumo character models.\nThis is purely cosmetic - no gameplay changes.\n\n## Model Requirements\n\n### Style\n- Low-poly (500-1000 triangles)\n- Cartoony/cute aesthetic\n- No rigging required (no animation)\n- Clear front/back direction\n\n### Size\n- Must fit within capsule collision bounds\n- Height ≈ 1.0 units\n- Width ≈ 1.0 units (capsule diameter)\n\n### Format\n- GLTF/GLB preferred (native Godot 4 support)\n- Alternative: OBJ, FBX\n\n## Options for Obtaining Models\n\n### Option A: Create in Blender\nSimple blob shape:\n1. Start with UV sphere\n2. Scale Y to elongate\n3. Add subdivisions\n4. Sculpt rounded bottom, smaller top\n5. Export as GLB\n\n### Option B: Free Assets\n- Kenney.nl (CC0)\n- Sketchfab (filter by license)\n- OpenGameArt\n\n### Option C: AI-Generated\n- Use text-to-3D tools\n- Ensure license allows use\n\n## Integration\n\n### Scene Changes\n```\nCharacterBody3D (sumo_agent.tscn)\n├── MeshInstance3D ← Replace CapsuleMesh with imported model\n│   └── sumo_blob.glb\n├── CollisionShape3D (keep CapsuleShape3D)\n└── AIController3D\n```\n\n### Material Setup\nTwo variants needed:\n- Agent 1: Blue-tinted material\n- Agent 2: Red-tinted material\n\n```gdscript\n# In sumo_agent.gd\n@export var agent_color: Color = Color.BLUE\n\nfunc _ready() -\u003e void:\n    var mesh = $MeshInstance3D\n    var material = mesh.get_active_material(0).duplicate()\n    material.albedo_color = agent_color\n    mesh.set_surface_override_material(0, material)\n```\n\n## Collision Consideration\n**Important**: Keep the capsule collision shape!\n- Smooth collision response\n- Predictable physics\n- Visual can differ from collision (common in games)\n\n## Testing\nAfter model swap:\n- [ ] Model loads without errors\n- [ ] Model is visible and oriented correctly\n- [ ] Different colors for each agent\n- [ ] Collision still works (unchanged)\n- [ ] Performance is acceptable\n\n## File Location\n```\nsumo-rl/\n├── models/\n│   └── sumo_blob.glb\n```\n\n## Acceptance Criteria\n- [ ] Model file exists in models/\n- [ ] Model imported into Godot without errors\n- [ ] Capsule mesh replaced with model\n- [ ] Both agents have distinct colors\n- [ ] Collision behavior unchanged\n- [ ] Game still plays correctly\n\n## Dependencies\n- Requires: Phase 3 Training success (SumoArena-9zx) - polish after core is working","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-06T20:28:24.39731-06:00","updated_at":"2025-12-06T20:28:24.39731-06:00","dependencies":[{"issue_id":"SumoArena-0v1","depends_on_id":"SumoArena-zw7","type":"parent-child","created_at":"2025-12-04T18:31:23.237911-06:00","created_by":"daemon"},{"issue_id":"SumoArena-0v1","depends_on_id":"SumoArena-9zx","type":"blocks","created_at":"2025-12-04T18:31:23.274776-06:00","created_by":"daemon"}]}
{"id":"SumoArena-1il","title":"Implement get_action() - action space","description":"# Implement get_action() - Action Space\n\n## What\nImplement the action handler that receives 2 continuous actions from the RL agent and applies them to agent movement.\n\n## Action Space (2 floats)\n\n| Index | Name | Range       | Description |\n|-------|------|-------------|-------------|\n| 0     | move | [-1.0, 1.0] | -1=backward, 0=stop, 1=forward |\n| 1     | turn | [-1.0, 1.0] | -1=left, 0=straight, 1=right |\n\n## Implementation\n\n```gdscript\n# In sumo_agent.gd\n\n# These are set by get_action callback or keyboard input\nvar input_move: float = 0.0\nvar input_turn: float = 0.0\n\nfunc get_action() -\u003e void:\n    \"\"\"\n    Called by AIController3D to receive actions from Python.\n    Actions are stored in ai_controller.action buffer.\n    \"\"\"\n    if not ai_controller or not is_controlled_by_ai:\n        return\n    \n    # Read actions from controller's action buffer\n    input_move = ai_controller.get_action_value(0)  # Index 0: move\n    input_turn = ai_controller.get_action_value(1)  # Index 1: turn\n    \n    # Clamp to valid range (in case of numerical issues)\n    input_move = clamp(input_move, -1.0, 1.0)\n    input_turn = clamp(input_turn, -1.0, 1.0)\n\nfunc _physics_process(delta: float) -\u003e void:\n    # Get input from either AI or keyboard\n    if is_controlled_by_ai:\n        get_action()\n    else:\n        get_keyboard_input()\n    \n    # Apply movement (same code for both input sources)\n    apply_movement(delta)\n\nfunc apply_movement(delta: float) -\u003e void:\n    # Apply rotation\n    rotate_y(input_turn * TURN_SPEED * delta)\n    \n    # Calculate forward force\n    var move_direction = -transform.basis.z  # Forward is -Z\n    var acceleration = (input_move * MOVE_FORCE) / MASS\n    velocity += move_direction * acceleration * delta\n    \n    # Apply friction\n    velocity *= (1.0 - FRICTION)\n    \n    # Clamp speed\n    var horiz_vel = Vector3(velocity.x, 0, velocity.z)\n    if horiz_vel.length() \u003e MAX_SPEED:\n        horiz_vel = horiz_vel.normalized() * MAX_SPEED\n        velocity.x = horiz_vel.x\n        velocity.z = horiz_vel.z\n    \n    # Gravity\n    velocity.y -= GRAVITY * delta\n    \n    # Move\n    move_and_slide()\n```\n\n## AIController3D Integration\n\nThe godot_rl_agents plugin uses callbacks. Check the plugin's API for exact method signatures:\n\n```gdscript\n# Possible callback pattern (verify with plugin docs)\nfunc _on_ai_controller_action_received(actions: Array) -\u003e void:\n    input_move = actions[0]\n    input_turn = actions[1]\n```\n\nOr it might use a polling pattern:\n```gdscript\n# Actions are pre-set in ai_controller before _physics_process\nfunc get_action() -\u003e void:\n    input_move = ai_controller.action[0]\n    input_turn = ai_controller.action[1]\n```\n\n## Action Smoothing (Optional)\nConsider smoothing abrupt action changes:\n```gdscript\nconst ACTION_SMOOTH: float = 0.8\n\nfunc get_action() -\u003e void:\n    var raw_move = ai_controller.action[0]\n    var raw_turn = ai_controller.action[1]\n    \n    # Exponential smoothing\n    input_move = lerp(input_move, raw_move, ACTION_SMOOTH)\n    input_turn = lerp(input_turn, raw_turn, ACTION_SMOOTH)\n```\n\nThis can make movement look more natural and prevent jittering.\nConsider: May slow down agent reactions. Start without smoothing.\n\n## Why Continuous Actions\nDiscrete actions (forward/stop/back) would work but:\n- Continuous allows fine-grained speed control\n- More nuanced strategies possible\n- PPO handles continuous actions well\n- More natural for physical control\n\n## Testing\nBefore full training:\n1. Connect Python with random actions\n2. Verify agents move randomly but smoothly\n3. Check action values are in expected range\n\n## Acceptance Criteria\n- [ ] get_action() reads 2 floats from AIController3D\n- [ ] input_move and input_turn are set correctly\n- [ ] Movement applies actions as expected\n- [ ] Actions are clamped to [-1, 1]\n- [ ] Works with keyboard fallback disabled\n\n## Dependencies\n- Requires: AIController3D added (SumoArena-486)\n- Requires: Movement system working (SumoArena-205)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.398137-06:00","updated_at":"2025-12-06T20:28:24.398137-06:00","closed_at":"2025-12-05T11:28:17.560855-06:00","dependencies":[{"issue_id":"SumoArena-1il","depends_on_id":"SumoArena-zzd","type":"parent-child","created_at":"2025-12-04T18:06:08.091162-06:00","created_by":"daemon"},{"issue_id":"SumoArena-1il","depends_on_id":"SumoArena-486","type":"blocks","created_at":"2025-12-04T18:06:08.137039-06:00","created_by":"daemon"}]}
{"id":"SumoArena-1pm","title":"Add swing attack mechanic (arms)","description":"Add directional swing attack with 5x push power, 120° arc, 2s cooldown. Includes arm meshes, animation, keyboard controls (Q/E, U/O), and AI action space update.","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T20:46:25.654832-06:00","updated_at":"2025-12-06T21:03:01.025829-06:00","closed_at":"2025-12-06T21:03:01.025829-06:00"}
{"id":"SumoArena-1q8","title":"Record demo video of training progression","description":"# Record Demo Video\n\n## What\nCreate a video showcasing the training progression and final trained behavior.\nEssential for sharing the project and demonstrating success.\n\n## Video Concept\n\n### Training Progression Montage\nShow evolution of agent behavior:\n1. **Random baseline**: 10 seconds of untrained agents\n2. **50k steps**: Early learning, basic movement\n3. **200k steps**: Contact-seeking, beginning to push\n4. **500k steps**: Strategic play, edge exploitation\n5. **Final model**: Best learned behavior\n\n### Side-by-Side Comparison\nSplit screen:\n- Left: Random/early checkpoint\n- Right: Trained agent\nSame initial conditions, different outcomes\n\n## Recording Setup\n\n### Method 1: OBS Studio (Recommended)\n1. Install OBS Studio\n2. Add Window Capture for Godot\n3. Record at 1080p 60fps\n4. Output format: MP4 or MKV\n\n### Method 2: Godot Movie Maker\nBuilt into Godot 4:\n1. Project Settings → Editor → Movie Writer\n2. Set output path\n3. Run scene - automatically records\n\n### Method 3: Screen Recording\n- macOS: QuickTime → File → New Screen Recording\n- Windows: Win+G → Game Bar\n- Linux: Simple Screen Recorder\n\n## Recording Checklist\n\n### Per Checkpoint\nFor each training stage, record:\n- [ ] 5-10 complete episodes\n- [ ] At least 1 dramatic finish\n- [ ] Some variety of outcomes\n\n### Settings\n- [ ] Fixed camera angle (consistent across clips)\n- [ ] Good lighting in scene\n- [ ] No debug prints visible (or cleared)\n- [ ] Godot window maximized\n\n## Video Editing\n\n### Tools\n- Free: DaVinci Resolve, OpenShot, Kdenlive\n- Quick: iMovie (macOS), ClipChamp (Windows)\n\n### Structure\n```\n0:00 - Title card \"Sumo RL\"\n0:05 - Random baseline (text: \"Untrained\")\n0:20 - Training montage (text: \"Training...\")\n0:40 - 50k checkpoint\n1:00 - 200k checkpoint\n1:20 - 500k checkpoint (text: \"500,000 steps\")\n1:40 - Best matches compilation\n2:00 - End card with links/credits\n```\n\n### Text Overlays\n- Training step counter\n- \"Untrained\" / \"Trained\" labels\n- Episode reward (optional)\n\n### Music (Optional)\n- Royalty-free background music\n- Builds excitement\n- Check licensing\n\n## TensorBoard Graphs\nInclude screenshots:\n- Reward curve over training\n- Episode length curve\n- Shows quantitative improvement\n\n## Output Specifications\n- Resolution: 1920x1080 or higher\n- Frame rate: 60fps preferred\n- Format: MP4 (H.264)\n- Length: 2-3 minutes ideal\n\n## Distribution\n- YouTube (unlisted or public)\n- Twitter/X (sub-2:20 for native)\n- GitHub README as GIF\n- Project documentation\n\n## Acceptance Criteria\n- [ ] Video recorded showing progression\n- [ ] Clear difference between early/late training\n- [ ] Video is watchable and interesting\n- [ ] Exported in shareable format\n- [ ] Optional: Uploaded somewhere accessible\n\n## Dependencies\n- Requires: Trained model with checkpoints (SumoArena-3ay)\n- Requires: Phase 3 validation passed (SumoArena-9zx)\n- Nice to have: Polish done first (SumoArena-0v1, SumoArena-i3l, SumoArena-46s)","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-06T20:28:24.398837-06:00","updated_at":"2025-12-06T20:28:24.398837-06:00","dependencies":[{"issue_id":"SumoArena-1q8","depends_on_id":"SumoArena-zw7","type":"parent-child","created_at":"2025-12-04T18:32:51.63078-06:00","created_by":"daemon"},{"issue_id":"SumoArena-1q8","depends_on_id":"SumoArena-9zx","type":"blocks","created_at":"2025-12-04T18:32:51.670032-06:00","created_by":"daemon"}]}
{"id":"SumoArena-205","title":"Implement agent movement script (sumo_agent.gd)","description":"# Implement Agent Movement Script\n\n## What\nCreate the movement and physics script: `scripts/sumo_agent.gd`\nHandles both keyboard input (for testing) and RL actions (for training).\n\n## Core Movement System\n\n### Physics Model\nWe're using a force-based acceleration model, not direct velocity control:\n- Agents have mass (affects collision response)\n- Movement applies force → acceleration → velocity\n- Velocity is capped at max_speed\n- Natural deceleration via friction/drag\n\n### Movement Code Pattern\n```gdscript\nextends CharacterBody3D\n\n# Physics constants\nconst MASS: float = 10.0\nconst MOVE_FORCE: float = 50.0\nconst MAX_SPEED: float = 8.0\nconst TURN_SPEED: float = 3.0\nconst FRICTION: float = 0.1  # Velocity decay per frame\n\n# Input (from keyboard or RL)\nvar input_move: float = 0.0  # -1 to 1\nvar input_turn: float = 0.0  # -1 to 1\n\nfunc _physics_process(delta: float) -\u003e void:\n    # Apply rotation\n    rotate_y(input_turn * TURN_SPEED * delta)\n    \n    # Calculate acceleration from force (F = ma → a = F/m)\n    var acceleration = (input_move * MOVE_FORCE) / MASS\n    \n    # Apply to velocity in facing direction\n    var move_direction = -transform.basis.z  # Forward is -Z in Godot\n    velocity += move_direction * acceleration * delta\n    \n    # Apply friction\n    velocity *= (1.0 - FRICTION)\n    \n    # Clamp to max speed\n    var horizontal_velocity = Vector3(velocity.x, 0, velocity.z)\n    if horizontal_velocity.length() \u003e MAX_SPEED:\n        horizontal_velocity = horizontal_velocity.normalized() * MAX_SPEED\n        velocity.x = horizontal_velocity.x\n        velocity.z = horizontal_velocity.z\n    \n    # Apply gravity\n    velocity.y -= 9.8 * delta\n    \n    # Move and slide handles collision response\n    move_and_slide()\n```\n\n### Input Modes\nThe script must support two input modes:\n1. **Human mode**: Read from Input singleton (WASD or arrows)\n2. **RL mode**: Read from AIController3D node\n\n```gdscript\nvar is_controlled_by_ai: bool = false\nvar ai_controller: Node = null  # Set in Phase 2\n\nfunc get_input() -\u003e void:\n    if is_controlled_by_ai and ai_controller:\n        input_move = ai_controller.move_action\n        input_turn = ai_controller.turn_action\n    else:\n        # Keyboard input for testing\n        input_move = Input.get_axis(\"move_backward\", \"move_forward\")\n        input_turn = Input.get_axis(\"turn_right\", \"turn_left\")\n```\n\n## Collision Response\nCharacterBody3D's move_and_slide() handles basic collision, but we need to add push mechanics:\n\n```gdscript\nfunc _physics_process(delta: float) -\u003e void:\n    # ... movement code ...\n    move_and_slide()\n    \n    # Handle collisions (push other agents)\n    for i in get_slide_collision_count():\n        var collision = get_slide_collision(i)\n        var collider = collision.get_collider()\n        \n        if collider is CharacterBody3D:\n            # Calculate push force based on our velocity\n            var push_direction = collision.get_normal() * -1\n            var push_strength = velocity.length() * 0.5  # Tune this\n            collider.apply_push(push_direction * push_strength)\n\nfunc apply_push(push_vector: Vector3) -\u003e void:\n    velocity += push_vector\n```\n\n## Input Mapping\nNeed to set up these input actions in project settings:\n- move_forward (W or Up)\n- move_backward (S or Down)\n- turn_left (A or Left)\n- turn_right (D or Right)\n\nFor two-player testing:\n- Player 1: WASD\n- Player 2: Arrow keys\n\n## Why This Approach\n1. **Force-based movement**: More realistic physics, momentum matters\n2. **Manual push handling**: CharacterBody3D doesn't transfer momentum automatically\n3. **Dual input support**: Same code works for human testing and RL\n\n## Acceptance Criteria\n- [ ] Script exists at scripts/sumo_agent.gd\n- [ ] Agent moves forward/backward based on input\n- [ ] Agent rotates left/right based on input\n- [ ] Movement respects max_speed\n- [ ] Friction applies when not moving\n- [ ] Gravity affects agent\n- [ ] Collisions with other agents push them\n- [ ] Push strength depends on velocity (momentum)\n\n## Dependencies\n- Requires: Agent scene (SumoArena-5j6) - needs the CharacterBody3D to attach to","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.399497-06:00","updated_at":"2025-12-06T20:28:24.399497-06:00","closed_at":"2025-12-04T19:39:09.486438-06:00","dependencies":[{"issue_id":"SumoArena-205","depends_on_id":"SumoArena-7ws","type":"parent-child","created_at":"2025-12-04T17:58:30.779016-06:00","created_by":"daemon"},{"issue_id":"SumoArena-205","depends_on_id":"SumoArena-5j6","type":"blocks","created_at":"2025-12-04T17:58:30.822845-06:00","created_by":"daemon"}]}
{"id":"SumoArena-3ay","title":"Run headless training (500k+ steps)","description":"# Run Headless Training\n\n## What\nExecute the main training run without visualization for maximum speed.\nTarget: 500k+ timesteps to see meaningful learned behavior.\n\n## Headless Setup\n\n### Godot Headless Mode\n```bash\n# Run Godot without window (faster)\ngodot --path /path/to/sumo-rl --headless \u0026\n```\n\nOr configure in Godot:\n- Project Settings → Display → Window → Size\n- Set \"Test Width/Height\" smaller for faster rendering\n- Or use \"Headless\" mode if available\n\n### Python Training\n```bash\ncd /path/to/sumo-rl/python\nsource venv/bin/activate\npython train.py --timesteps 500000\n```\n\n## Parallel Environments (Faster)\nFor faster training, run multiple Godot instances:\n\n### Method 1: Multiple Instances (Simple)\n```bash\n# Terminal 1\ngodot --path /path/to/sumo-rl --headless \u0026\n\n# Terminal 2\npython train.py --n_envs 8 --timesteps 500000\n```\n\nNote: godot_rl_agents handles multi-env internally.\n\n### Method 2: Explicit Parallel (Advanced)\nSee godot_rl_agents docs for running separate processes.\n\n## Training Duration\nRough estimates (varies by hardware):\n\n| Timesteps | Single Env | 8 Envs |\n|-----------|------------|--------|\n| 100k | ~15-30 min | ~5-10 min |\n| 500k | ~1-2 hours | ~20-40 min |\n| 1M | ~2-4 hours | ~40-80 min |\n\n## Monitoring During Training\n\n### TensorBoard\n```bash\n# In separate terminal\ntensorboard --logdir runs/\n# Open http://localhost:6006\n```\n\nKey metrics to watch:\n- **ep_reward_mean**: Main success metric\n- **ep_len_mean**: Episode length (shorter = faster wins)\n- **explained_variance**: Should be high (model predicts well)\n\n### Progress Output\nTraining script shows:\n- Current timestep\n- Recent episode rewards\n- Loss values\n\n## Expected Learning Progression\n\n### 0-50k steps: Random Exploration\n- Agents flail randomly\n- Occasional accidental wins\n- Win rate ≈ 50/50 (random)\n\n### 50k-100k steps: Basic Movement\n- Agents start moving toward opponent\n- Still mostly random engagement\n- Episode length may decrease\n\n### 100k-200k steps: Contact Seeking\n- Agents reliably find each other\n- Beginning to push (not just touch)\n- Win rate shifts based on position\n\n### 200k-500k steps: Edge Exploitation\n- Agents learn to push toward edge\n- Position matters more\n- Seeing intentional strategies\n\n### 500k+ steps: Refined Tactics\n- Baiting, feinting\n- Momentum management\n- Edge awareness (both self and opponent)\n\n## Checkpoint Strategy\nDefault: Save every 50k steps\nAdjust with `--checkpoint_freq`\n\nCheckpoints allow:\n- Resume after crash\n- Compare behavior at different training stages\n- Find optimal stopping point (before overfitting)\n\n## Troubleshooting\n\n### Training stuck (no progress)\n- Check Godot is running\n- Verify socket connection\n- Look for errors in Godot console\n\n### Rewards not improving\n- Check reward function is correct\n- May need more steps (500k is minimum for this task)\n- Check observations are updating\n\n### Memory issues\n- Reduce n_steps if running out of RAM\n- Close other applications\n- Consider smaller batch_size\n\n## Acceptance Criteria\n- [ ] Training runs to completion (500k+ steps)\n- [ ] Multiple checkpoints saved\n- [ ] TensorBoard shows reward improvement\n- [ ] No crashes or errors\n- [ ] Final model saved\n\n## Dependencies\n- Requires: Visualization test passed (SumoArena-vvy)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.400192-06:00","updated_at":"2025-12-07T12:51:33.990115-06:00","closed_at":"2025-12-07T12:51:33.990115-06:00","dependencies":[{"issue_id":"SumoArena-3ay","depends_on_id":"SumoArena-4s1","type":"parent-child","created_at":"2025-12-04T18:21:21.241571-06:00","created_by":"daemon"},{"issue_id":"SumoArena-3ay","depends_on_id":"SumoArena-vvy","type":"blocks","created_at":"2025-12-04T18:21:21.279279-06:00","created_by":"daemon"}]}
{"id":"SumoArena-46s","title":"Add sound effects (impacts, fall)","description":"# Add Sound Effects\n\n## What\nAdd audio feedback for key game events.\nEnhances the experience for demos and makes gameplay more satisfying.\n\n## Sounds Needed\n\n### 1. Collision Impact\n- **When**: Agents collide\n- **Style**: Meaty thud/bump\n- **Pitch**: Vary based on collision speed\n- **Volume**: Scale with impact force\n\n### 2. Fall Sound\n- **When**: Agent falls off platform\n- **Style**: Comedic boing/splash/woosh\n- **One-shot**: Single sound\n\n### 3. Round Start (Optional)\n- **When**: Episode resets\n- **Style**: Ding/bell\n- **Purpose**: Audio cue for new round\n\n### 4. Ambient/Crowd (Optional)\n- **Continuous**: Low background ambiance\n- **Style**: Distant crowd, arena atmosphere\n- **Volume**: Very low, unobtrusive\n\n## Implementation\n\n### AudioStreamPlayer Setup\n```gdscript\n# In sumo_agent.gd\n\n@onready var impact_sound: AudioStreamPlayer3D = $ImpactSound\n@onready var fall_sound: AudioStreamPlayer = $FallSound\n\nfunc play_impact(velocity: float) -\u003e void:\n    # Pitch varies with speed (faster = higher pitch)\n    impact_sound.pitch_scale = remap(velocity, 0, MAX_SPEED, 0.8, 1.2)\n    # Volume varies with speed\n    impact_sound.volume_db = remap(velocity, 0, MAX_SPEED, -10, 0)\n    impact_sound.play()\n\nfunc _on_fell_off() -\u003e void:\n    fall_sound.play()\n```\n\n### Arena Audio\n```gdscript\n# In arena_manager.gd\n\n@onready var round_bell: AudioStreamPlayer = $RoundBell\n\nfunc reset_episode() -\u003e void:\n    round_bell.play()\n    # ... rest of reset\n```\n\n## Sound Sources\n\n### Option A: Free Sound Libraries\n- Freesound.org (check licenses)\n- OpenGameArt.org\n- Kenney.nl/assets (CC0)\n\n### Option B: Generate/Record\n- BFXR/SFXR for retro game sounds\n- Record and process real sounds\n- Text-to-sound AI tools\n\n### Recommended Sounds\n- **Impact**: Punching bag hit, body slam\n- **Fall**: Cartoon fall whistle + splash\n- **Bell**: Boxing ring bell, ding\n\n## Audio Bus Setup (Optional)\nFor volume control:\n1. Create \"SFX\" audio bus\n2. Route all effects to SFX bus\n3. Master control for all sound effects\n\n## Scene Structure\n```\nCharacterBody3D (sumo_agent.tscn)\n├── MeshInstance3D\n├── CollisionShape3D\n├── AIController3D\n├── DustParticles\n├── ImpactSound (AudioStreamPlayer3D) ← NEW\n└── FallSound (AudioStreamPlayer) ← NEW (2D is fine for this)\n```\n\n## Audio Files Location\n```\nsumo-rl/\n├── audio/\n│   ├── impact.wav\n│   ├── fall.wav\n│   └── bell.wav\n```\n\n## Audio Settings\n- Format: WAV or OGG (OGG for longer sounds)\n- Sample rate: 44100 Hz\n- Bit depth: 16-bit\n- Mono is fine for effects\n\n## Testing\n- [ ] Impact sound plays on collision\n- [ ] Impact sound varies with speed\n- [ ] Fall sound plays when falling\n- [ ] No audio clipping or distortion\n- [ ] Sounds don't overlap badly\n- [ ] Volume levels are balanced\n\n## Acceptance Criteria\n- [ ] At least impact and fall sounds working\n- [ ] Sounds are appropriate/not annoying\n- [ ] Audio enhances experience\n- [ ] No performance issues\n\n## Dependencies\n- Requires: Phase 3 complete\n- Nice to have: Particle effects first (audio+visual sync)","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-06T20:28:24.400862-06:00","updated_at":"2025-12-06T20:28:24.400862-06:00","dependencies":[{"issue_id":"SumoArena-46s","depends_on_id":"SumoArena-zw7","type":"parent-child","created_at":"2025-12-04T18:32:22.224885-06:00","created_by":"daemon"},{"issue_id":"SumoArena-46s","depends_on_id":"SumoArena-9zx","type":"blocks","created_at":"2025-12-04T18:32:22.263016-06:00","created_by":"daemon"}]}
{"id":"SumoArena-486","title":"Add AIController3D to agent scene","description":"# Add AIController3D to Agent Scene\n\n## What\nAdd the AIController3D node from godot_rl_agents to the sumo_agent.tscn scene.\nThis node acts as the bridge between the Godot agent and Python training.\n\n## Updated Scene Structure\n```\nCharacterBody3D (sumo_agent.tscn root)\n├── MeshInstance3D (CapsuleMesh)\n├── CollisionShape3D (CapsuleShape3D)\n└── AIController3D ← NEW\n```\n\n## AIController3D Configuration\nThe node needs to be configured for our action/observation spaces:\n\n### Properties to Set\n- **n_actions**: 2 (move and turn)\n- **action_space**: Continuous\n- **reset_after**: Episodes controlled by arena, not per-agent timeout\n\n## How AIController3D Works\n1. Python sends actions via socket → AIController3D receives them\n2. Agent script reads actions from AIController3D\n3. Agent computes observations → writes to AIController3D\n4. Agent computes reward → writes to AIController3D\n5. AIController3D sends obs/reward to Python\n\n## Integration Points\nThe agent script needs to:\n1. Get a reference to the AIController3D node\n2. Implement `get_obs()` callback\n3. Implement `get_action()` callback  \n4. Implement `get_reward()` callback\n\n```gdscript\n# In sumo_agent.gd\n@onready var ai_controller: AIController3D = $AIController3D\n\nfunc _ready() -\u003e void:\n    if ai_controller:\n        ai_controller.init(self)\n        is_controlled_by_ai = true\n```\n\n## Control Mode Toggle\nNeed to handle both AI and human control:\n```gdscript\nvar is_controlled_by_ai: bool = false\n\nfunc _physics_process(delta: float) -\u003e void:\n    if is_controlled_by_ai:\n        # Actions come from AIController3D (set in get_action callback)\n        pass\n    else:\n        # Read keyboard input\n        get_keyboard_input()\n    \n    apply_movement(delta)\n```\n\n## Testing the Integration\nAfter adding AIController3D:\n- Scene should still load without errors\n- Human control should still work (keyboard fallback)\n- AIController3D should be visible in scene tree\n\n## Acceptance Criteria\n- [ ] AIController3D is a child of sumo_agent\n- [ ] Node is configured for 2 continuous actions\n- [ ] Agent script has reference to AIController3D\n- [ ] Human control still works (regression test)\n- [ ] Scene loads without errors\n\n## Dependencies\n- Requires: Plugin installed (SumoArena-chj)\n- Requires: Agent scene exists (SumoArena-5j6)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.401496-06:00","updated_at":"2025-12-06T20:28:24.401496-06:00","closed_at":"2025-12-05T11:28:17.559051-06:00","dependencies":[{"issue_id":"SumoArena-486","depends_on_id":"SumoArena-zzd","type":"parent-child","created_at":"2025-12-04T18:01:05.883207-06:00","created_by":"daemon"},{"issue_id":"SumoArena-486","depends_on_id":"SumoArena-chj","type":"blocks","created_at":"2025-12-04T18:01:05.926714-06:00","created_by":"daemon"}]}
{"id":"SumoArena-4ip","title":"Implement fall detection","description":"# Implement Fall Detection\n\n## What\nDetect when an agent falls off the platform and emit a signal.\nThis is critical for episode termination and reward assignment.\n\n## Detection Method\nSimple Y-position check:\n```gdscript\n# In sumo_agent.gd\nsignal fell_off\n\nconst FALL_THRESHOLD: float = -1.0\n\nfunc _physics_process(delta: float) -\u003e void:\n    # ... movement code ...\n    \n    # Check if we've fallen\n    if global_position.y \u003c FALL_THRESHOLD:\n        emit_signal(\"fell_off\")\n```\n\n## Why Y \u003c -1.0?\n- Platform surface is at Y = 0\n- Agent center is at Y ≈ 0.5 (half capsule height)\n- Threshold at -1.0 gives buffer for falling motion\n- Not too low (don't want long fall time)\n- Not too high (avoid false positives from collision bounces)\n\n## Signal Architecture\nThe `fell_off` signal propagates up to the arena manager:\n\n```\nAgent emits fell_off\n    ↓\nArena manager receives signal\n    ↓\nArena manager determines winner/loser\n    ↓\nArena manager ends episode\n    ↓\n[Phase 2] Arena manager reports rewards to AIControllers\n```\n\n## Arena Manager Connection\nIn arena_manager.gd (or training_arena.tscn script):\n```gdscript\nfunc _ready() -\u003e void:\n    $Agent1.fell_off.connect(_on_agent1_fell)\n    $Agent2.fell_off.connect(_on_agent2_fell)\n\nfunc _on_agent1_fell() -\u003e void:\n    end_episode(winner=2, loser=1)\n\nfunc _on_agent2_fell() -\u003e void:\n    end_episode(winner=1, loser=2)\n\nfunc end_episode(winner: int, loser: int) -\u003e void:\n    print(\"Agent %d wins! Agent %d fell off.\" % [winner, loser])\n    # Reset will be added later\n```\n\n## Edge Cases to Handle\n1. **Both fall simultaneously**: Very rare, but possible. Treat as draw or award to whoever fell last (based on which signal arrives first).\n\n2. **Stuck below platform**: If agent somehow gets under platform without triggering fall, they'll be stuck. The Y check prevents this.\n\n3. **False positive from bounce**: If collision pushes agent momentarily below Y=0 but they recover, -1.0 threshold avoids false trigger.\n\n## Visual Feedback (Optional)\nFor debugging/demo purposes:\n- Print to console when fall detected\n- Later: particle effect, sound\n\n## Testing\nTo test fall detection:\n1. Run scene with one agent\n2. Walk off the edge\n3. Verify signal is emitted\n4. Verify console prints detection\n\n## Acceptance Criteria\n- [ ] Agent emits fell_off signal when Y \u003c -1.0\n- [ ] Signal is emitted only once per fall\n- [ ] Arena can connect to and receive signal\n- [ ] Console prints when agent falls\n- [ ] Works for both agents independently\n\n## Dependencies\n- Requires: Movement script (SumoArena-205) - needs _physics_process in place\n- Requires: Training arena (SumoArena-xyc) - needs arena to receive signals","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.402132-06:00","updated_at":"2025-12-06T20:28:24.402132-06:00","closed_at":"2025-12-04T19:40:40.488308-06:00","dependencies":[{"issue_id":"SumoArena-4ip","depends_on_id":"SumoArena-7ws","type":"parent-child","created_at":"2025-12-04T17:59:22.843497-06:00","created_by":"daemon"},{"issue_id":"SumoArena-4ip","depends_on_id":"SumoArena-205","type":"blocks","created_at":"2025-12-04T17:59:22.879-06:00","created_by":"daemon"},{"issue_id":"SumoArena-4ip","depends_on_id":"SumoArena-xyc","type":"blocks","created_at":"2025-12-04T17:59:22.914314-06:00","created_by":"daemon"}]}
{"id":"SumoArena-4s1","title":"EPIC: Phase 3 - Training Pipeline","description":"# Phase 3: Training Pipeline\n\n## Overview\nThis epic covers the Python-side training infrastructure. The goal is a working training loop that produces agents demonstrating learned sumo strategies.\n\n## Algorithm Choice: PPO\nPPO (Proximal Policy Optimization) is chosen because:\n1. **Stable**: Clipped objective prevents catastrophic policy updates\n2. **Sample Efficient**: Better than vanilla policy gradient\n3. **Proven**: Works well on continuous control tasks\n4. **godot_rl_agents Compatible**: Native support via StableBaselines3\n\n## Training Strategy\n1. **Start Small**: 1-2 environments with visualization to debug\n2. **Scale Up**: 8-16 parallel environments for faster training\n3. **Monitor**: TensorBoard for loss curves, reward trends\n4. **Checkpoint**: Save models periodically to track progression\n\n## Expected Learning Progression\nBased on similar environments:\n- **0-50k steps**: Random flailing, occasional accidental wins\n- **50k-100k steps**: Agents learn to move toward opponent\n- **100k-300k steps**: Agents learn to push (not just touch)\n- **300k-500k steps**: Basic edge exploitation (push toward edge)\n- **500k+**: Advanced strategies (baiting, momentum management)\n\n## Hyperparameter Starting Points\n- Learning rate: 3e-4 (SB3 default, works for most tasks)\n- Batch size: 64-128\n- n_steps: 2048 (enough for full episodes)\n- Entropy coefficient: 0.01 (encourage exploration initially)\n- Clip range: 0.2 (standard)\n\n## Evaluation Strategy\n- Periodic evaluation vs frozen checkpoints\n- Record videos at milestones for qualitative assessment\n- Track win rate, average episode length, reward curves\n\n## Dependencies\nRequires Phase 2 (RL Integration) - need the Godot env to be RL-ready.","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-06T20:28:24.402785-06:00","updated_at":"2025-12-07T12:51:33.99062-06:00","closed_at":"2025-12-07T12:51:33.99062-06:00","dependencies":[{"issue_id":"SumoArena-4s1","depends_on_id":"SumoArena-590","type":"blocks","created_at":"2025-12-04T18:33:04.073271-06:00","created_by":"daemon"}]}
{"id":"SumoArena-590","title":"VALIDATE: Phase 2 RL integration","description":"# Validation: Phase 2 RL Integration\n\n## What\nComprehensive testing of the RL integration before moving to training.\nThis validation ensures Godot is ready to communicate with Python.\n\n## Test Protocol\n\n### Test 1: Plugin Installation\n1. Open project in Godot editor\n2. Check Project Settings → Plugins\n3. Verify:\n   - [ ] godot_rl_agents plugin is listed\n   - [ ] Plugin is enabled (checkbox checked)\n   - [ ] No errors in Output panel\n\n### Test 2: Scene Structure\n1. Open training_arena.tscn\n2. Verify scene tree:\n   - [ ] Sync node present\n   - [ ] Both agents have AIController3D child\n   - [ ] arena_manager.gd attached to root\n\n### Test 3: Observations\n1. Run scene\n2. Add debug print for observations:\n   ```gdscript\n   func _physics_process(delta):\n       if Input.is_action_just_pressed(\"ui_accept\"):\n           print(\"Agent 1 obs: \", agent1.get_obs())\n           print(\"Agent 2 obs: \", agent2.get_obs())\n   ```\n3. Verify:\n   - [ ] get_obs() returns 9 floats\n   - [ ] Values are in expected ranges\n   - [ ] Observations change when agents move\n\n### Test 4: Keyboard Control Still Works\n1. Run scene without Python connected\n2. Verify:\n   - [ ] Keyboard controls still work (fallback mode)\n   - [ ] Human can play the game\n   - [ ] No errors about missing AI controller\n\n### Test 5: Reset Cycle\n1. Walk one agent off the edge\n2. Verify:\n   - [ ] Win/loss is detected\n   - [ ] Both agents reset to spawn\n   - [ ] Can play another round immediately\n   - [ ] No stale state from previous episode\n\n### Test 6: Timeout\n1. Set MAX_STEPS to a low value (e.g., 100)\n2. Run scene, don't push anyone off\n3. Verify:\n   - [ ] Timeout triggers after 100 steps\n   - [ ] Both agents get draw state\n   - [ ] Reset occurs correctly\n   - [ ] Reset MAX_STEPS to 1000\n\n### Test 7: Sync Node Ready\n1. Run scene\n2. Check console output\n3. Verify:\n   - [ ] Sync node reports waiting for connection (or similar)\n   - [ ] No errors from Sync node\n   - [ ] Scene runs normally without Python\n\n### Test 8: Multiple Episodes\n1. Run scene for 5+ episodes (manually trigger falls)\n2. Verify:\n   - [ ] All episodes reset correctly\n   - [ ] No memory leaks (performance stays stable)\n   - [ ] Counters work correctly\n\n## Quick Python Test (Optional)\nIf comfortable with Python, quick validation:\n```bash\npip install godot-rl\npython -c \"from godot_rl.wrappers.stable_baselines_wrapper import StableBaselinesGodotEnv; print('OK')\"\n```\n\n## Common Issues and Fixes\n\n| Issue | Fix |\n|-------|-----|\n| \"AIController3D not found\" | Check node name matches reference |\n| Observations return null | Check enemy reference is set |\n| Reset doesn't work | Check signal connections |\n| Sync node errors | Check plugin is enabled |\n\n## Completion Criteria\n- [ ] All 8 tests pass\n- [ ] No errors in console during play\n- [ ] Multiple episodes run smoothly\n- [ ] Ready for Python training script\n\n## Dependencies\n- Requires: All Phase 2 tasks complete\n  - Arena manager (SumoArena-bqv)\n  - Sync node (SumoArena-o53)\n  - Reset logic (SumoArena-6v9)\n  - get_reward (SumoArena-vv3)\n  - get_action (SumoArena-1il)\n  - get_obs (SumoArena-ubo)\n  - AIController3D (SumoArena-486)\n  - Plugin install (SumoArena-chj)\n\n## Notes\nThis is a BLOCKING validation. Do not proceed to Phase 3 until all tests pass.\nThe Python training script assumes a working Godot environment.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-06T20:28:24.403392-06:00","updated_at":"2025-12-06T20:28:24.403392-06:00","closed_at":"2025-12-05T12:56:32.464841-06:00","dependencies":[{"issue_id":"SumoArena-590","depends_on_id":"SumoArena-zzd","type":"parent-child","created_at":"2025-12-04T18:14:44.179944-06:00","created_by":"daemon"},{"issue_id":"SumoArena-590","depends_on_id":"SumoArena-bqv","type":"blocks","created_at":"2025-12-04T18:14:44.215457-06:00","created_by":"daemon"},{"issue_id":"SumoArena-590","depends_on_id":"SumoArena-o53","type":"blocks","created_at":"2025-12-04T18:14:44.250128-06:00","created_by":"daemon"},{"issue_id":"SumoArena-590","depends_on_id":"SumoArena-n4r","type":"blocks","created_at":"2025-12-05T12:50:09.960188-06:00","created_by":"daemon"}]}
{"id":"SumoArena-5j6","title":"Create agent scene (sumo_agent.tscn)","description":"# Create Agent Scene\n\n## What\nCreate a reusable agent scene: `scenes/sumo_agent.tscn`\nThis will be instanced twice in the arena for self-play.\n\n## Technical Specifications\n- **Root Node**: CharacterBody3D\n- **Visual**: CapsuleShape (height 1.0, radius 0.5)\n- **Collision**: CapsuleShape3D matching visual\n\n## Scene Structure\n```\nCharacterBody3D (sumo_agent.tscn root)\n├── MeshInstance3D (CapsuleMesh)\n│   └── height: 1.0, radius: 0.5\n├── CollisionShape3D (CapsuleShape3D)\n│   └── height: 1.0, radius: 0.5\n└── [Later: AIController3D - added in Phase 2]\n```\n\n## Why CharacterBody3D (not RigidBody3D)\nCharacterBody3D gives us:\n- Precise control via move_and_slide()\n- No unwanted physics (rolling, tipping over)\n- Predictable collision response\n- Can still push/be pushed via code\n\nRigidBody3D would require:\n- Locking rotations (capsule wants to roll)\n- Fighting with physics to get desired behavior\n- More complex collision handling\n\n## Physics Properties (stored in script, not scene)\nThese will be defined in sumo_agent.gd:\n- mass: 10.0 (used for collision response calculation)\n- move_force: 50.0 (acceleration when moving)\n- max_speed: 8.0 (velocity cap)\n- turn_speed: 3.0 rad/sec\n\n## Collision Setup\n- Layer 2 (Agents): This agent is on layer 2\n- Mask 1 (Environment): Collide with platform\n- Mask 2 (Agents): Collide with other agents\n\n## Visual Appearance\n- Simple colored material (distinguish agent 1 vs 2)\n- Agent 1: Blue-ish\n- Agent 2: Red-ish\n- Can be parameterized via exported variable\n\n## Why Capsule Shape\n- Smooth collision response (no corners to snag)\n- Natural representation of a standing figure\n- Easy to replace with character model later (same bounding shape)\n\n## Acceptance Criteria\n- [ ] Agent scene exists at scenes/sumo_agent.tscn\n- [ ] CharacterBody3D as root\n- [ ] Capsule mesh visible (1.0 height, 0.5 radius)\n- [ ] Collision shape matches mesh\n- [ ] Can be instanced multiple times\n- [ ] Different instances can have different colors\n\n## Dependencies\n- Requires: Project directory structure (SumoArena-xuk)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.404032-06:00","updated_at":"2025-12-06T20:28:24.404032-06:00","closed_at":"2025-12-04T19:03:06.662643-06:00","dependencies":[{"issue_id":"SumoArena-5j6","depends_on_id":"SumoArena-7ws","type":"parent-child","created_at":"2025-12-04T17:57:59.341422-06:00","created_by":"daemon"},{"issue_id":"SumoArena-5j6","depends_on_id":"SumoArena-xuk","type":"blocks","created_at":"2025-12-04T17:57:59.377201-06:00","created_by":"daemon"}]}
{"id":"SumoArena-60n","title":"EPIC: Combat Rebalancing","description":"Rebalance charge vs swing attacks. Charge becomes the devastating commit move (2x speed, 4x push). Swing becomes a spacing tool (2x push). Fixes swing collision issues.","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-06T21:54:10.051699-06:00","updated_at":"2025-12-06T21:59:40.29916-06:00","closed_at":"2025-12-06T21:59:40.29916-06:00"}
{"id":"SumoArena-63l","title":"Set up input actions for keyboard testing","description":"# Set Up Input Actions for Keyboard Testing\n\n## What\nConfigure Godot's InputMap with actions for two-player keyboard controls.\nThis allows human testing of the physics before RL integration.\n\n## Input Actions to Define\nIn Project Settings → Input Map:\n\n### Player 1 (WASD)\n| Action Name    | Key      |\n|----------------|----------|\n| p1_forward     | W        |\n| p1_backward    | S        |\n| p1_turn_left   | A        |\n| p1_turn_right  | D        |\n\n### Player 2 (Arrow Keys)\n| Action Name    | Key        |\n|----------------|------------|\n| p2_forward     | Up Arrow   |\n| p2_backward    | Down Arrow |\n| p2_turn_left   | Left Arrow |\n| p2_turn_right  | Right Arrow|\n\n## Alternative: Per-Agent Configuration\nInstead of hardcoding p1/p2 prefixes, we could use a single set of actions and configure per-agent:\n\n```gdscript\n# In sumo_agent.gd\n@export var player_id: int = 1\n\nfunc get_keyboard_input() -\u003e void:\n    var forward_action = \"p%d_forward\" % player_id\n    var backward_action = \"p%d_backward\" % player_id\n    # ... etc\n```\n\n## Why Separate Actions Per Player\n- Allows two humans to play simultaneously\n- Cleaner than dynamically building action names\n- Easier to test agent-vs-agent physics\n\n## Implementation\nEither:\n1. Edit project.godot directly (InputMap section)\n2. Use Godot editor: Project → Project Settings → Input Map\n\n## Script Integration\n```gdscript\n# In sumo_agent.gd\n@export var player_id: int = 1\n\nfunc get_keyboard_input() -\u003e void:\n    if player_id == 1:\n        input_move = Input.get_axis(\"p1_backward\", \"p1_forward\")\n        input_turn = Input.get_axis(\"p1_turn_right\", \"p1_turn_left\")\n    else:\n        input_move = Input.get_axis(\"p2_backward\", \"p2_forward\")\n        input_turn = Input.get_axis(\"p2_turn_right\", \"p2_turn_left\")\n```\n\n## Testing Checklist\nAfter setup:\n- [ ] Launch training_arena.tscn\n- [ ] Agent 1 responds to WASD\n- [ ] Agent 2 responds to Arrow keys\n- [ ] Both agents can be controlled simultaneously\n- [ ] Controls feel responsive\n\n## Acceptance Criteria\n- [ ] All 8 input actions defined in project settings\n- [ ] Agent script reads correct inputs based on player_id\n- [ ] Two players can control agents simultaneously\n- [ ] Input actions saved in project.godot\n\n## Dependencies\n- Requires: Movement script (SumoArena-205) - script needs to use these actions","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.404665-06:00","updated_at":"2025-12-06T20:28:24.404665-06:00","closed_at":"2025-12-04T19:45:35.732809-06:00","dependencies":[{"issue_id":"SumoArena-63l","depends_on_id":"SumoArena-7ws","type":"parent-child","created_at":"2025-12-04T17:59:45.75896-06:00","created_by":"daemon"},{"issue_id":"SumoArena-63l","depends_on_id":"SumoArena-205","type":"blocks","created_at":"2025-12-04T17:59:45.79406-06:00","created_by":"daemon"}]}
{"id":"SumoArena-691","title":"VALIDATE: Phase 1 physics and gameplay","description":"# Validation: Phase 1 Physics and Gameplay\n\n## What\nManual testing checkpoint to verify all Phase 1 mechanics work correctly before moving to RL integration.\n\n## Test Protocol\n\n### Test 1: Basic Movement\n1. Run training_arena.tscn\n2. Control Agent 1 with WASD\n3. Verify:\n   - [ ] Forward/backward movement works\n   - [ ] Turning left/right works\n   - [ ] Movement feels responsive (not floaty, not sluggish)\n   - [ ] Agent stays upright (no tilting/rolling)\n\n### Test 2: Two-Player Controls\n1. Both players use their controls simultaneously\n2. Verify:\n   - [ ] Both agents respond independently\n   - [ ] No input conflicts\n\n### Test 3: Collision Physics\n1. Walk one agent into the other\n2. Verify:\n   - [ ] Agents don't pass through each other\n   - [ ] Collision pushes the other agent\n   - [ ] Running into opponent pushes harder than walking\n\n### Test 4: Momentum Transfer\n1. Build up speed (hold W for 1-2 seconds)\n2. Collide with stationary opponent\n3. Verify:\n   - [ ] Faster agent pushes opponent farther\n   - [ ] There's a noticeable difference vs walking speed\n\n### Test 5: Edge Behavior\n1. Walk to the edge of the platform\n2. Verify:\n   - [ ] Agent can walk off the edge (no invisible wall)\n   - [ ] Agent falls when off edge\n   - [ ] Agent doesn't slide off unexpectedly\n\n### Test 6: Fall Detection\n1. Walk off the edge intentionally\n2. Verify:\n   - [ ] fell_off signal is emitted\n   - [ ] Console shows \"Agent X fell off\" message\n   - [ ] Winner is correctly identified\n\n### Test 7: Gameplay Feel\n1. Play a few \"matches\" against yourself or another person\n2. Evaluate:\n   - [ ] Arena size feels right (not too big/small)\n   - [ ] Matches end in reasonable time\n   - [ ] Pushing opponent off is achievable but not trivial\n   - [ ] There's room for skill/strategy\n\n## Tuning Adjustments\nIf tests reveal problems, adjust these parameters:\n\n| Symptom | Adjustment |\n|---------|------------|\n| Agents too slow | Increase MAX_SPEED or MOVE_FORCE |\n| Agents too fast | Decrease MAX_SPEED |\n| Collisions too weak | Increase push multiplier |\n| Collisions too bouncy | Decrease push multiplier, add friction |\n| Arena too big (boring) | Reduce platform radius |\n| Arena too small (chaotic) | Increase platform radius |\n\n## Completion Criteria\n- [ ] All 7 tests pass\n- [ ] No major bugs found\n- [ ] Gameplay feels fun and fair\n- [ ] Ready for RL integration\n\n## Dependencies\n- Requires: All Phase 1 tasks complete\n  - Fall detection (SumoArena-4ip)\n  - Input actions (SumoArena-63l)\n  - Training arena (SumoArena-xyc)\n  - Movement script (SumoArena-205)\n  - Agent scene (SumoArena-5j6)\n  - Platform scene (SumoArena-8ay)\n  - Directory structure (SumoArena-xuk)\n\n## Notes\nThis is a BLOCKING validation. Do not proceed to Phase 2 until all tests pass. \nFix any issues before moving forward.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-06T20:28:24.405289-06:00","updated_at":"2025-12-06T20:28:24.405289-06:00","closed_at":"2025-12-05T11:12:50.251201-06:00","dependencies":[{"issue_id":"SumoArena-691","depends_on_id":"SumoArena-7ws","type":"parent-child","created_at":"2025-12-04T18:00:12.335516-06:00","created_by":"daemon"},{"issue_id":"SumoArena-691","depends_on_id":"SumoArena-4ip","type":"blocks","created_at":"2025-12-04T18:00:12.37112-06:00","created_by":"daemon"},{"issue_id":"SumoArena-691","depends_on_id":"SumoArena-63l","type":"blocks","created_at":"2025-12-04T18:00:12.407143-06:00","created_by":"daemon"}]}
{"id":"SumoArena-6v9","title":"Implement episode reset logic","description":"# Implement Episode Reset Logic\n\n## What\nImplement the reset function that returns agents to starting positions when an episode ends.\nProper reset is critical for training stability.\n\n## Reset Triggers\n1. Agent falls off (win/loss)\n2. Timeout reached (max steps, draw)\n3. Manual reset (debugging)\n\n## What Must Be Reset\n\n### Per Agent\n- Position: Return to spawn point\n- Rotation: Face opponent\n- Velocity: Zero out\n- Accumulated reward: Clear\n- Episode ended flag: Clear\n\n### Per Arena\n- Step counter: Zero\n- Episode state: Active\n\n## Implementation\n\n### Agent Reset\n```gdscript\n# In sumo_agent.gd\n\nvar spawn_position: Vector3\nvar spawn_rotation: float\n\nfunc _ready() -\u003e void:\n    # Save initial spawn state\n    spawn_position = global_position\n    spawn_rotation = rotation.y\n\nfunc reset() -\u003e void:\n    \"\"\"Reset agent to starting state.\"\"\"\n    # Position and rotation\n    global_position = spawn_position\n    rotation = Vector3(0, spawn_rotation, 0)\n    \n    # Physics state\n    velocity = Vector3.ZERO\n    \n    # RL state\n    accumulated_reward = 0.0\n    episode_ended = false\n    input_move = 0.0\n    input_turn = 0.0\n    \n    # Tell AIController3D we've reset\n    if ai_controller:\n        ai_controller.reset()\n```\n\n### Arena Reset\n```gdscript\n# In arena_manager.gd\n\nconst MAX_STEPS: int = 1000\nvar current_step: int = 0\n\nfunc reset_episode() -\u003e void:\n    \"\"\"Reset everything for a new episode.\"\"\"\n    current_step = 0\n    \n    # Reset both agents\n    agent1.reset()\n    agent2.reset()\n    \n    # Reset alive flags\n    agent1_alive = true\n    agent2_alive = true\n    \n    print(\"Episode reset. New round!\")\n\nfunc _physics_process(delta: float) -\u003e void:\n    current_step += 1\n    \n    # Check for timeout\n    if current_step \u003e= MAX_STEPS:\n        _on_timeout()\n\nfunc end_episode() -\u003e void:\n    \"\"\"Called after win/loss/draw determined.\"\"\"\n    # Small delay before reset (optional, for visual feedback)\n    await get_tree().create_timer(0.5).timeout\n    reset_episode()\n```\n\n## Sync Node Integration\nThe godot_rl_agents Sync node coordinates resets:\n\n```gdscript\n# The Sync node handles:\n# 1. Collecting obs from all AIControllers\n# 2. Sending actions to all AIControllers\n# 3. Detecting when any agent requests reset\n# 4. Coordinating reset across all agents\n```\n\nKey point: Both agents must reset simultaneously, not independently.\n\n## Reset Timing Considerations\n\n### Option A: Immediate Reset\n```gdscript\nfunc end_episode() -\u003e void:\n    reset_episode()  # Immediate\n```\nPro: Simple\nCon: Hard to see what happened\n\n### Option B: Delayed Reset\n```gdscript\nfunc end_episode() -\u003e void:\n    await get_tree().create_timer(0.5).timeout\n    reset_episode()\n```\nPro: Can see the fall, useful for debugging\nCon: Slightly more complex, tiny training overhead\n\n### Option C: Reset on Next Step (Recommended for RL)\nLet the Sync node handle reset timing:\n```gdscript\nfunc end_episode() -\u003e void:\n    # Set done flag, Sync will reset on next step\n    agent1.episode_ended = true\n    agent2.episode_ended = true\n```\nPro: Clean integration with RL loop\nCon: Need to understand Sync node behavior\n\n## Edge Case: Reset While Falling\nIf reset is called while agent is mid-fall:\n```gdscript\nfunc reset() -\u003e void:\n    # Force position even if currently moving\n    global_position = spawn_position\n    velocity = Vector3.ZERO\n```\nThe position assignment overrides physics state.\n\n## Testing Reset\n1. Start episode\n2. Push one agent off\n3. Verify both agents return to spawn\n4. Verify velocities are zero\n5. Verify episode counter resets\n6. Repeat several times to ensure consistency\n\n## Common Reset Bugs to Avoid\n1. **Velocity not cleared**: Agent slides on reset\n2. **Rotation wrong**: Agents face wrong direction\n3. **Only one agent resets**: Desync between agents\n4. **Reward not cleared**: Old rewards bleed into new episode\n5. **Reset during physics**: Use call_deferred if needed\n\n## Acceptance Criteria\n- [ ] Agents return to spawn positions on reset\n- [ ] Agents face each other after reset\n- [ ] Velocities are zeroed\n- [ ] Accumulated rewards are cleared\n- [ ] Episode step counter resets\n- [ ] Both agents reset simultaneously\n- [ ] Can run many episodes without issues\n\n## Dependencies\n- Requires: Fall detection (SumoArena-4ip) - triggers reset\n- Requires: get_reward (SumoArena-vv3) - reward cleared on reset\n- Requires: AIController3D (SumoArena-486) - for ai_controller.reset()","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.405882-06:00","updated_at":"2025-12-06T20:28:24.405882-06:00","closed_at":"2025-12-05T12:13:06.21023-06:00","dependencies":[{"issue_id":"SumoArena-6v9","depends_on_id":"SumoArena-zzd","type":"parent-child","created_at":"2025-12-04T18:09:42.261515-06:00","created_by":"daemon"},{"issue_id":"SumoArena-6v9","depends_on_id":"SumoArena-vv3","type":"blocks","created_at":"2025-12-04T18:09:42.296648-06:00","created_by":"daemon"}]}
{"id":"SumoArena-70y","title":"Create Python training script (train.py)","description":"# Create Python Training Script\n\n## What\nCreate the main training script: `python/train.py`\nThis script connects to Godot, trains the PPO agent, and saves checkpoints.\n\n## Training Script Implementation\n\n```python\n# python/train.py\n\"\"\"\nSumo RL Training Script\n\nTrains two agents to compete in sumo-style matches using self-play.\nBoth agents share the same policy weights.\n\"\"\"\n\nimport os\nimport argparse\nfrom datetime import datetime\n\nfrom godot_rl.wrappers.stable_baselines_wrapper import StableBaselinesGodotEnv\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\nfrom stable_baselines3.common.vec_env import VecMonitor\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Train Sumo RL agents\")\n    parser.add_argument(\"--timesteps\", type=int, default=500_000,\n                        help=\"Total training timesteps\")\n    parser.add_argument(\"--n_envs\", type=int, default=1,\n                        help=\"Number of parallel environments\")\n    parser.add_argument(\"--checkpoint_freq\", type=int, default=50_000,\n                        help=\"Save checkpoint every N steps\")\n    parser.add_argument(\"--viz\", action=\"store_true\",\n                        help=\"Enable visualization (slower)\")\n    parser.add_argument(\"--resume\", type=str, default=None,\n                        help=\"Path to model to resume training from\")\n    parser.add_argument(\"--seed\", type=int, default=42,\n                        help=\"Random seed\")\n    return parser.parse_args()\n\ndef make_env(n_envs: int = 1, viz: bool = True):\n    \"\"\"Create the Godot environment.\"\"\"\n    env = StableBaselinesGodotEnv(\n        env_path=None,  # None = connect to running Godot instance\n        show_window=viz,\n        n_parallel=n_envs,\n        seed=42,\n    )\n    return VecMonitor(env)  # Adds episode statistics\n\ndef main():\n    args = parse_args()\n    \n    # Create run directory\n    run_name = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_dir = f\"runs/{run_name}\"\n    os.makedirs(run_dir, exist_ok=True)\n    os.makedirs(f\"{run_dir}/checkpoints\", exist_ok=True)\n    \n    print(f\"Starting training run: {run_name}\")\n    print(f\"Timesteps: {args.timesteps:,}\")\n    print(f\"Checkpoints saved to: {run_dir}/checkpoints\")\n    \n    # Create environment\n    print(\"Connecting to Godot...\")\n    env = make_env(n_envs=args.n_envs, viz=args.viz)\n    print(f\"Environment created: obs_space={env.observation_space}, act_space={env.action_space}\")\n    \n    # Create or load model\n    if args.resume:\n        print(f\"Resuming from: {args.resume}\")\n        model = PPO.load(args.resume, env=env)\n    else:\n        model = PPO(\n            \"MlpPolicy\",\n            env,\n            verbose=1,\n            learning_rate=3e-4,\n            n_steps=2048,\n            batch_size=64,\n            n_epochs=10,\n            gamma=0.99,\n            gae_lambda=0.95,\n            clip_range=0.2,\n            ent_coef=0.01,\n            tensorboard_log=f\"{run_dir}/tensorboard\",\n            seed=args.seed,\n        )\n    \n    # Callbacks\n    checkpoint_callback = CheckpointCallback(\n        save_freq=args.checkpoint_freq // args.n_envs,\n        save_path=f\"{run_dir}/checkpoints\",\n        name_prefix=\"sumo_ppo\",\n    )\n    \n    # Train\n    print(\"Starting training...\")\n    model.learn(\n        total_timesteps=args.timesteps,\n        callback=checkpoint_callback,\n        progress_bar=True,\n    )\n    \n    # Save final model\n    final_path = f\"{run_dir}/sumo_final\"\n    model.save(final_path)\n    print(f\"Final model saved to: {final_path}\")\n    \n    env.close()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Usage Examples\n\n### Quick Test (Visualized)\n```bash\n# Terminal 1: Start Godot\ngodot --path /path/to/sumo-rl\n\n# Terminal 2: Run training\ncd python\npython train.py --timesteps 10000 --viz\n```\n\n### Full Training (Headless)\n```bash\n# Godot headless (faster)\ngodot --path /path/to/sumo-rl --headless\n\n# Train without visualization\npython train.py --timesteps 500000\n```\n\n### Resume Training\n```bash\npython train.py --resume runs/20240101_120000/checkpoints/sumo_ppo_100000_steps --timesteps 500000\n```\n\n## Hyperparameters Explained\n\n| Parameter | Value | Rationale |\n|-----------|-------|-----------|\n| learning_rate | 3e-4 | SB3 default, good starting point |\n| n_steps | 2048 | Enough for multiple full episodes |\n| batch_size | 64 | Standard for continuous control |\n| n_epochs | 10 | Number of gradient updates per batch |\n| gamma | 0.99 | High discount for episodic tasks |\n| gae_lambda | 0.95 | GAE smoothing parameter |\n| clip_range | 0.2 | Standard PPO clip |\n| ent_coef | 0.01 | Encourages exploration |\n\n## Self-Play Note\nIn godot_rl_agents with multiple AIController3D nodes:\n- Both agents appear as separate \"sub-environments\"\n- Python sees 2x the observations/actions\n- Same policy weights are used (parameter sharing)\n- This naturally implements self-play\n\n## Acceptance Criteria\n- [ ] train.py script created and runs without error\n- [ ] Can connect to running Godot instance\n- [ ] Training loop starts and shows progress\n- [ ] Checkpoints are saved at specified intervals\n- [ ] TensorBoard logs are created\n- [ ] Final model is saved\n\n## Dependencies\n- Requires: Python environment set up (SumoArena-ulj)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.406519-06:00","updated_at":"2025-12-06T20:28:24.406519-06:00","closed_at":"2025-12-05T13:12:43.637975-06:00","dependencies":[{"issue_id":"SumoArena-70y","depends_on_id":"SumoArena-4s1","type":"parent-child","created_at":"2025-12-04T18:20:28.471584-06:00","created_by":"daemon"},{"issue_id":"SumoArena-70y","depends_on_id":"SumoArena-ulj","type":"blocks","created_at":"2025-12-04T18:20:28.507619-06:00","created_by":"daemon"}]}
{"id":"SumoArena-7ws","title":"EPIC: Phase 1 - Basic Scene Setup","description":"# Phase 1: Basic Scene Setup\n\n## Overview\nThis epic covers the foundational Godot scene creation and physics validation. The goal is to have a playable prototype where two capsule agents can push each other around a circular platform using keyboard controls, with proper fall detection.\n\n## Why This Phase First\nBefore any RL integration, we need to validate that the core game mechanics work correctly:\n- Physics collisions feel right (not too bouncy, not too sticky)\n- Momentum affects push strength as expected\n- Fall detection is reliable\n- The arena size creates interesting gameplay\n\n## Success Criteria\n- Two capsules visible on a circular platform\n- WASD controls one agent, arrow keys control the other\n- Collisions push agents realistically\n- Agents fall off the edge and this is detected\n- Running into opponent pushes harder than walking\n\n## Technical Decisions\n- Using CharacterBody3D (not RigidBody3D) for predictable, controllable physics\n- CSGCylinder3D for platform (easy to prototype, can replace with mesh later)\n- Capsule collision shapes (stable, no corner cases)\n\n## Dependencies\nNone - this is the foundation phase.\n\n## Estimated Complexity\nMedium - straightforward Godot work but requires physics tuning.","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-06T20:28:24.407122-06:00","updated_at":"2025-12-06T20:51:56.494529-06:00","closed_at":"2025-12-06T20:51:56.494529-06:00"}
{"id":"SumoArena-8ay","title":"Create circular platform scene (platform.tscn)","description":"# Create Circular Platform Scene\n\n## What\nCreate the arena platform as a reusable scene: `scenes/platform.tscn`\n\n## Technical Specifications\n- **Shape**: Cylinder (circular disc)\n- **Radius**: 6.0 units\n- **Thickness**: 0.5 units\n- **Position**: Centered at origin, top surface at Y=0\n\n## Implementation Options\n\n### Option A: CSGCylinder3D (Recommended for prototype)\n```gdscript\nCSGCylinder3D:\n  radius: 6.0\n  height: 0.5\n  position: Vector3(0, -0.25, 0)  # Top surface at Y=0\n  use_collision: true\n```\nPros: Quick to set up, built-in collision\nCons: Less control over collision shape\n\n### Option B: MeshInstance3D + CollisionShape3D\n```gdscript\nStaticBody3D:\n  MeshInstance3D (CylinderMesh):\n    radius: 6.0\n    height: 0.5\n  CollisionShape3D (CylinderShape3D):\n    radius: 6.0\n    height: 0.5\n```\nPros: More control, better for production\nCons: More setup\n\n## Visual Appearance\n- Material: Simple gray or tan color\n- Consider adding a subtle ring texture later (Phase 4)\n- Edge should be visually distinct (darker rim or line)\n\n## Collision Considerations\n- StaticBody3D for the platform (doesn't move)\n- Collision layer: Environment (layer 1)\n- Collision mask: Agents (layer 2)\n\n## Why These Dimensions\n- Radius 6.0 with agent radius 0.5 = 12 agent-widths across\n- Big enough for maneuvering, small enough for frequent engagement\n- Can be tuned later based on training behavior\n\n## Acceptance Criteria\n- [ ] Platform scene exists at scenes/platform.tscn\n- [ ] Platform is circular, radius 6.0\n- [ ] Platform has collision enabled\n- [ ] Can be instanced in other scenes\n\n## Dependencies\n- Requires: Project directory structure (SumoArena-xuk)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.407721-06:00","updated_at":"2025-12-06T20:28:24.407721-06:00","closed_at":"2025-12-04T18:48:35.838511-06:00","dependencies":[{"issue_id":"SumoArena-8ay","depends_on_id":"SumoArena-7ws","type":"parent-child","created_at":"2025-12-04T17:57:39.111287-06:00","created_by":"daemon"},{"issue_id":"SumoArena-8ay","depends_on_id":"SumoArena-xuk","type":"blocks","created_at":"2025-12-04T17:57:39.146737-06:00","created_by":"daemon"}]}
{"id":"SumoArena-9zx","title":"VALIDATE: Phase 3 training success","description":"# Validation: Phase 3 Training Success\n\n## What\nValidate that training has produced agents with meaningful learned behavior.\nThis is the core success criteria for the entire project.\n\n## Success Criteria\n\n### Minimum Viable Success\n- [ ] Training ran for 500k+ steps without crashes\n- [ ] TensorBoard shows reward improvement over time\n- [ ] Agents consistently move toward opponent\n- [ ] Agents attempt to push (not just touch)\n\n### Solid Success\n- [ ] Agents push opponent toward edge\n- [ ] Agents avoid falling off edge themselves\n- [ ] Episode length decreases over training\n- [ ] Behavior is different from random baseline\n\n### Excellent Success\n- [ ] Agents show strategic behavior (baiting, positioning)\n- [ ] Agents recover from near-edge positions\n- [ ] Momentum is used intentionally\n- [ ] Behavior is entertaining to watch\n\n## Quantitative Checkpoints\n\n### At 100k steps\n- [ ] Mean episode reward \u003e random baseline\n- [ ] Agents move toward each other (not away)\n\n### At 300k steps\n- [ ] Visible pushing behavior\n- [ ] Some edge exploitation visible\n\n### At 500k steps\n- [ ] Consistent strategies emerge\n- [ ] Episode length notably shorter than timeout\n\n## Testing Protocol\n\n### 1. TensorBoard Analysis\n```bash\ntensorboard --logdir runs/\n```\nCheck curves for:\n- Upward trend in ep_reward_mean\n- Downward trend in ep_len_mean\n- Stable (not exploding) loss values\n\n### 2. Checkpoint Comparison\n```bash\n# Early vs late comparison\npython eval.py runs/latest/checkpoints/sumo_ppo_50000_steps --episodes 10\npython eval.py runs/latest/checkpoints/sumo_ppo_500000_steps --episodes 10\n```\n\nExpect to see:\n- Different behavior\n- Later checkpoint more decisive\n- Shorter episodes in later checkpoint\n\n### 3. Visual Observation\nWatch 10+ episodes of the final model:\n```bash\npython eval.py runs/latest/sumo_final --episodes 10 --slow\n```\n\nLook for:\n- [ ] Purposeful movement (not random jittering)\n- [ ] Contact-seeking behavior\n- [ ] Pushing after contact\n- [ ] Edge awareness\n\n### 4. Random Baseline Comparison\nRun a quick sanity check with random actions:\n```python\n# In Python\nimport numpy as np\n# Random action policy: np.random.uniform(-1, 1, size=2)\n# Compare episode lengths and outcomes\n```\n\nTrained agent should:\n- Win more often against random\n- Have shorter episodes than random vs random\n\n## Common Failure Modes\n\n### 1. Agents don't engage\n**Symptom**: Agents wander or stand still\n**Causes**: \n- Step penalty too low (no urgency)\n- Observations not updating correctly\n- Actions not applied\n\n### 2. Agents fall off immediately\n**Symptom**: Very short episodes, random-looking\n**Causes**:\n- Rewards inverted (penalizing wins)\n- Fall detection broken\n- Physics too chaotic\n\n### 3. No improvement after 500k steps\n**Symptom**: Flat learning curve\n**Causes**:\n- Learning rate too low/high\n- Reward signal too sparse\n- Bug in observation or action implementation\n\n### 4. Overfitting to specific pattern\n**Symptom**: Same exact behavior every episode\n**Causes**:\n- Entropy coefficient too low\n- Not enough exploration\n- Could also be genuine convergence (okay)\n\n## If Training Failed\nRemediation steps:\n1. Check Phase 2 validation tests again\n2. Add debug prints to verify obs/action flow\n3. Try simpler reward shaping temporarily\n4. Reduce environment complexity\n5. Check godot_rl_agents examples for comparison\n\n## Acceptance Criteria\n- [ ] Training completed 500k+ steps\n- [ ] Clear improvement visible in TensorBoard\n- [ ] Visual behavior is non-random\n- [ ] Agents show pushing/edge-exploitation\n- [ ] Project goals achieved\n\n## Dependencies\n- Requires: Evaluation script (SumoArena-pi6)\n- Requires: Headless training (SumoArena-3ay)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-06T20:28:24.408319-06:00","updated_at":"2025-12-07T12:51:33.990381-06:00","closed_at":"2025-12-07T12:51:33.990381-06:00","dependencies":[{"issue_id":"SumoArena-9zx","depends_on_id":"SumoArena-4s1","type":"parent-child","created_at":"2025-12-04T18:30:50.943687-06:00","created_by":"daemon"},{"issue_id":"SumoArena-9zx","depends_on_id":"SumoArena-pi6","type":"blocks","created_at":"2025-12-04T18:30:50.987001-06:00","created_by":"daemon"}]}
{"id":"SumoArena-bao","title":"Add agent team glow","description":"Add subtle emissive glow around each agent matching their team color.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T20:46:41.470625-06:00","updated_at":"2025-12-06T21:03:22.014141-06:00","closed_at":"2025-12-06T21:03:22.014141-06:00"}
{"id":"SumoArena-ber","title":"Add charge/dash ability","description":"# Add Charge/Dash Ability\n\n## Overview\nAdd a discrete \"charge\" action that gives a burst of speed, creating risk/reward gameplay decisions.\n\n## Mechanics\n\n### Charge Action\n- **Trigger**: Discrete action (0 = no charge, 1 = charge)\n- **Effect**: Burst of forward velocity (2-3x normal max speed)\n- **Duration**: ~0.3 seconds of boosted movement\n- **Cooldown**: 1-2 seconds before can charge again\n- **Commitment**: Cannot turn (or reduced turn rate) while charging\n\n### Why This Creates Interesting Gameplay\n1. **Risk/reward**: Charge commits you to a direction - miss and you might fly off the edge\n2. **Timing matters**: Charging into a braced/prepared opponent vs catching them off-guard\n3. **Mind games**: Feinting, baiting charges, dodging\n4. **Momentum**: A charging agent transfers more force on collision\n\n## Implementation\n\n### Action Space Changes\n```gdscript\n# Old: 2 continuous actions\n# move: -1 to 1\n# turn: -1 to 1\n\n# New: 2 continuous + 1 discrete\n# move: -1 to 1\n# turn: -1 to 1  \n# charge: 0 or 1 (discrete)\n```\n\n### Agent State\n```gdscript\nvar is_charging: bool = false\nvar charge_timer: float = 0.0\nvar charge_cooldown: float = 0.0\n\nconst CHARGE_DURATION: float = 0.3\nconst CHARGE_COOLDOWN: float = 1.5\nconst CHARGE_SPEED_MULT: float = 2.5\nconst CHARGE_TURN_MULT: float = 0.2  # Reduced turning while charging\n```\n\n### Physics Changes\n```gdscript\nfunc apply_movement(delta: float) -\u003e void:\n    # Update charge state\n    if charge_cooldown \u003e 0:\n        charge_cooldown -= delta\n    \n    if is_charging:\n        charge_timer -= delta\n        if charge_timer \u003c= 0:\n            is_charging = false\n    \n    # Check for new charge\n    if input_charge and not is_charging and charge_cooldown \u003c= 0:\n        is_charging = true\n        charge_timer = CHARGE_DURATION\n        charge_cooldown = CHARGE_COOLDOWN\n    \n    # Apply turn (reduced while charging)\n    var turn_mult = CHARGE_TURN_MULT if is_charging else 1.0\n    rotate_y(input_turn * TURN_SPEED * turn_mult * delta)\n    \n    # Apply movement (boosted while charging)\n    var speed_mult = CHARGE_SPEED_MULT if is_charging else 1.0\n    var acceleration = (input_move * MOVE_FORCE * speed_mult) / MASS\n    # ... rest of movement code\n```\n\n### Observation Space Changes\nAdd 2 new observations:\n```gdscript\n# Add to get_obs():\nobs.append(1.0 if is_charging else 0.0)  # Am I charging?\nobs.append(clamp(charge_cooldown / CHARGE_COOLDOWN, 0.0, 1.0))  # Cooldown remaining\n\n# Also add enemy charge state:\nobs.append(1.0 if enemy.is_charging else 0.0)  # Is enemy charging?\n```\n\nNew observation count: 9 + 3 = 12 floats\n\n### AI Controller Changes\n```gdscript\nfunc get_action_space() -\u003e Dictionary:\n    return {\n        \"move\": {\"size\": 1, \"action_type\": \"continuous\"},\n        \"turn\": {\"size\": 1, \"action_type\": \"continuous\"},\n        \"charge\": {\"size\": 2, \"action_type\": \"discrete\"},  # 0=no, 1=yes\n    }\n\nfunc set_action(action) -\u003e void:\n    sumo_agent.input_move = clamp(action[\"move\"][0], -1.0, 1.0)\n    sumo_agent.input_turn = clamp(action[\"turn\"][0], -1.0, 1.0)\n    sumo_agent.input_charge = action[\"charge\"] == 1\n```\n\n### Collision Changes\nCharging agents should transfer more force:\n```gdscript\nfunc apply_push(push_vector: Vector3) -\u003e void:\n    # Reduce push received if braced/grounded\n    # Increase push given if charging\n    velocity += push_vector\n```\n\n## Visual Feedback (Optional)\n- Change agent color while charging (e.g., glow red)\n- Speed lines / particle trail\n- Screen shake on charge collision\n\n## Tuning Parameters\n| Parameter | Initial Value | Notes |\n|-----------|---------------|-------|\n| CHARGE_DURATION | 0.3s | How long the boost lasts |\n| CHARGE_COOLDOWN | 1.5s | Time before can charge again |\n| CHARGE_SPEED_MULT | 2.5x | Speed multiplier during charge |\n| CHARGE_TURN_MULT | 0.2x | Turn rate during charge (commitment) |\n\n## Testing Checklist\n- [ ] Charge activates on discrete action\n- [ ] Speed boost feels impactful\n- [ ] Cooldown prevents spam\n- [ ] Reduced turning creates commitment\n- [ ] Charging into opponent pushes harder\n- [ ] Missing a charge is punishable (can fall off)\n- [ ] AI can learn to use charge effectively\n\n## Dependencies\n- Requires updating sumo_agent.gd\n- Requires updating sumo_ai_controller.gd\n- Requires updating train.py (action space handling)\n","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T20:28:24.40887-06:00","updated_at":"2025-12-06T20:28:24.40887-06:00","closed_at":"2025-12-05T16:02:46.96073-06:00"}
{"id":"SumoArena-bqv","title":"Create arena_manager.gd script","description":"# Create Arena Manager Script\n\n## What\nCreate the arena manager script: `scripts/arena_manager.gd`\nThis script orchestrates episodes, tracks game state, and coordinates agents.\n\n## Responsibilities\n1. Connect agents to each other (set enemy references)\n2. Track episode step count\n3. Handle fall detection signals\n4. Determine win/loss/draw\n5. Trigger resets\n6. Timeout management\n\n## Implementation\n\n```gdscript\n# scripts/arena_manager.gd\nextends Node3D\n\n# Configuration\nconst MAX_STEPS: int = 1000\n\n# Agent references (set via @export or @onready)\n@onready var agent1: CharacterBody3D = $Agent1\n@onready var agent2: CharacterBody3D = $Agent2\n\n# Episode state\nvar current_step: int = 0\nvar agent1_alive: bool = true\nvar agent2_alive: bool = true\nvar episode_active: bool = true\n\nfunc _ready() -\u003e void:\n    # Connect agents to each other\n    agent1.enemy = agent2\n    agent2.enemy = agent1\n    \n    # Connect fall signals\n    agent1.fell_off.connect(_on_agent1_fell)\n    agent2.fell_off.connect(_on_agent2_fell)\n    \n    # Start first episode\n    reset_episode()\n\nfunc _physics_process(delta: float) -\u003e void:\n    if not episode_active:\n        return\n    \n    current_step += 1\n    \n    # Check timeout\n    if current_step \u003e= MAX_STEPS:\n        _on_timeout()\n\nfunc _on_agent1_fell() -\u003e void:\n    if not episode_active:\n        return\n    agent1_alive = false\n    check_episode_end()\n\nfunc _on_agent2_fell() -\u003e void:\n    if not episode_active:\n        return\n    agent2_alive = false\n    check_episode_end()\n\nfunc check_episode_end() -\u003e void:\n    if not agent1_alive and not agent2_alive:\n        # Both fell - draw (very rare)\n        print(\"Draw! Both agents fell.\")\n        agent1.on_draw()\n        agent2.on_draw()\n        end_episode()\n    elif not agent1_alive:\n        print(\"Agent 2 wins! Agent 1 fell.\")\n        agent1.on_lost()\n        agent2.on_won()\n        end_episode()\n    elif not agent2_alive:\n        print(\"Agent 1 wins! Agent 2 fell.\")\n        agent2.on_lost()\n        agent1.on_won()\n        end_episode()\n\nfunc _on_timeout() -\u003e void:\n    print(\"Timeout! Draw after %d steps.\" % MAX_STEPS)\n    agent1.on_draw()\n    agent2.on_draw()\n    end_episode()\n\nfunc end_episode() -\u003e void:\n    episode_active = false\n    # Small delay for visual feedback (optional)\n    await get_tree().create_timer(0.3).timeout\n    reset_episode()\n\nfunc reset_episode() -\u003e void:\n    print(\"--- New Episode ---\")\n    current_step = 0\n    agent1_alive = true\n    agent2_alive = true\n    episode_active = true\n    \n    agent1.reset()\n    agent2.reset()\n```\n\n## Attaching to Scene\nThe arena manager should be the root script of training_arena.tscn:\n```\nNode3D (training_arena.tscn) ← Attach arena_manager.gd here\n├── Sync\n├── Platform\n├── Agent1\n├── Agent2\n├── Camera3D\n└── DirectionalLight3D\n```\n\n## Debug Output\nConsole output helps track training progress:\n- \"--- New Episode ---\" on reset\n- \"Agent X wins!\" on win\n- \"Timeout!\" on max steps\n\nFor training, consider making prints optional:\n```gdscript\n@export var verbose: bool = false\n\nfunc print_debug(msg: String) -\u003e void:\n    if verbose:\n        print(msg)\n```\n\n## Statistics Tracking (Optional)\nFor monitoring training:\n```gdscript\nvar total_episodes: int = 0\nvar agent1_wins: int = 0\nvar agent2_wins: int = 0\nvar draws: int = 0\n\nfunc end_episode() -\u003e void:\n    total_episodes += 1\n    # Print stats every 100 episodes\n    if total_episodes % 100 == 0:\n        print(\"Episodes: %d, A1 wins: %d, A2 wins: %d, Draws: %d\" % [\n            total_episodes, agent1_wins, agent2_wins, draws\n        ])\n```\n\n## Integration with Sync Node\nThe Sync node handles most of the RL coordination.\nArena manager focuses on game logic, not RL communication.\n\n## Acceptance Criteria\n- [ ] Script exists at scripts/arena_manager.gd\n- [ ] Arena scene has script attached\n- [ ] Agents are connected (enemy references set)\n- [ ] Fall signals trigger win/loss\n- [ ] Timeout triggers draw\n- [ ] Reset works correctly\n- [ ] Can run multiple episodes in sequence\n\n## Dependencies\n- Requires: Training arena scene (SumoArena-xyc)\n- Requires: Reset logic (SumoArena-6v9)\n- Requires: Fall detection (SumoArena-4ip)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.409449-06:00","updated_at":"2025-12-06T20:28:24.409449-06:00","closed_at":"2025-12-05T12:13:06.211124-06:00","dependencies":[{"issue_id":"SumoArena-bqv","depends_on_id":"SumoArena-zzd","type":"parent-child","created_at":"2025-12-04T18:10:42.781478-06:00","created_by":"daemon"},{"issue_id":"SumoArena-bqv","depends_on_id":"SumoArena-6v9","type":"blocks","created_at":"2025-12-04T18:10:42.816694-06:00","created_by":"daemon"}]}
{"id":"SumoArena-c7m","title":"Add movement penalty while swinging","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T21:54:00.50257-06:00","updated_at":"2025-12-06T21:59:40.298135-06:00","closed_at":"2025-12-06T21:59:40.298135-06:00"}
{"id":"SumoArena-chj","title":"Install godot_rl_agents plugin","description":"# Install godot_rl_agents Plugin\n\n## What\nInstall the godot_rl_agents plugin into the Godot project.\nThis plugin provides the infrastructure for connecting Godot to Python-based RL training.\n\n## About godot_rl_agents\n- GitHub: https://github.com/edbeeching/godot_rl_agents\n- Provides: AIController3D, Sync node, GodotEnv Python wrapper\n- Compatible with: StableBaselines3, RLlib, CleanRL, Sample Factory\n\n## Installation Methods\n\n### Method 1: Asset Library (Recommended)\n1. Open Godot editor\n2. AssetLib tab → Search \"godot rl agents\"\n3. Download and install to addons/godot_rl_agents\n\n### Method 2: Manual Download\n1. Download from GitHub releases\n2. Extract to addons/godot_rl_agents/\n3. Ensure plugin.cfg is in addons/godot_rl_agents/\n\n## Post-Installation Setup\n1. Project → Project Settings → Plugins\n2. Find \"Godot RL Agents\"\n3. Enable the plugin\n\n## Verify Installation\nAfter enabling:\n- [ ] Can add AIController3D nodes\n- [ ] Can add Sync node\n- [ ] No errors in Output panel\n\n## Project Structure After Installation\n```\nsumo-rl/\n├── addons/\n│   └── godot_rl_agents/\n│       ├── plugin.cfg\n│       ├── sync.gd\n│       ├── controller/\n│       │   └── ai_controller_3d.gd\n│       └── ...\n```\n\n## Version Compatibility\n- Ensure plugin version matches Godot version (4.x for Godot 4)\n- Check godot_rl_agents GitHub for latest release notes\n\n## Python Side\nThe plugin also has a Python package:\n```bash\npip install godot-rl\n```\nThis will be set up in Phase 3 training tasks.\n\n## Acceptance Criteria\n- [ ] Plugin files exist in addons/godot_rl_agents/\n- [ ] Plugin is enabled in Project Settings\n- [ ] AIController3D node type is available\n- [ ] Sync node type is available\n- [ ] No errors on project load\n\n## Dependencies\n- Requires: Phase 1 Validation complete (SumoArena-691)\n- Requires: Directory structure (SumoArena-xuk) - addons folder exists","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.410055-06:00","updated_at":"2025-12-06T20:28:24.410055-06:00","closed_at":"2025-12-05T11:22:25.448416-06:00","dependencies":[{"issue_id":"SumoArena-chj","depends_on_id":"SumoArena-zzd","type":"parent-child","created_at":"2025-12-04T18:00:43.993127-06:00","created_by":"daemon"},{"issue_id":"SumoArena-chj","depends_on_id":"SumoArena-691","type":"blocks","created_at":"2025-12-04T18:00:44.029922-06:00","created_by":"daemon"}]}
{"id":"SumoArena-cil","title":"Multi-arena training scene","description":"# Multi-Arena Training Scene\n\n## Overview\nCreate a training scene with multiple arenas running in parallel within a single Godot instance. This dramatically speeds up training by collecting more experiences per frame.\n\n## Why This Helps\n- Current: 1 arena = 2 agents = 15 it/s\n- With 8 arenas: 8 arenas = 16 agents = ~120 it/s (8x faster)\n- Same Python connection handles all agents\n- No need to export game or run multiple processes\n\n## Implementation\n\n### New Scene: multi_training.tscn\n\n```\nMultiTrainingArena (Node3D)\n├── Sync (godot_rl_agents)\n├── Arena1 (Node3D) @ position (0, 0, 0)\n│   ├── Platform\n│   ├── Agent1 (player_id=1)\n│   ├── Agent2 (player_id=2)\n│   └── ArenaManager\n├── Arena2 (Node3D) @ position (20, 0, 0)\n│   ├── Platform\n│   ├── Agent1 (player_id=1)\n│   ├── Agent2 (player_id=2)\n│   └── ArenaManager\n├── Arena3 (Node3D) @ position (40, 0, 0)\n│   └── ...\n├── ... (repeat for N arenas)\n├── Camera3D (overhead view of all arenas)\n└── Lighting\n```\n\n### Key Points\n\n1. **Spacing**: Each arena is offset by ~20 units (arena diameter is 12) so they dont interfere\n\n2. **Independent ArenaManagers**: Each arena has its own manager handling resets for just its 2 agents\n\n3. **Single Sync Node**: One Sync at the root collects from ALL AIController3D nodes in the tree\n\n4. **Enemy References**: Each agent must reference its arena-mate, not agents from other arenas\n\n### Creating the Scene\n\n**Option A: Manual in Editor**\n1. Create new scene `multi_training.tscn`\n2. Add Sync node from godot_rl_agents\n3. Instance `training_arena.tscn` multiple times\n4. Offset each instance position.x by 20\n5. Add single overhead camera\n\n**Option B: Procedural Generation**\n```gdscript\n# multi_arena_spawner.gd\nextends Node3D\n\n@export var num_arenas: int = 8\n@export var arena_spacing: float = 20.0\nvar arena_scene = preload(\"res://scenes/training_arena.tscn\")\n\nfunc _ready():\n    for i in num_arenas:\n        var arena = arena_scene.instantiate()\n        arena.position.x = i * arena_spacing\n        arena.name = \"Arena%d\" % (i + 1)\n        add_child(arena)\n```\n\n### ArenaManager Changes\nCurrently arena_manager.gd might have hardcoded references. Update to:\n```gdscript\n# Find agents as children, not global paths\n@onready var agent1 = $Agent1\n@onready var agent2 = $Agent2\n```\n\n### Camera Setup\nFor debugging/visualization, an overhead camera that sees all arenas:\n```gdscript\n# Position camera to see all arenas\n# With 8 arenas spread 20 units apart = 140 unit span\n# Camera at center (70, 50, 0) looking down\ntransform = Transform3D.looking_at(Vector3(70, 0, 0), Vector3.UP).translated(Vector3(70, 50, 0))\n```\n\nOr orthographic camera for cleaner view.\n\n### Testing\n1. Create scene with 2 arenas first\n2. Run training, verify both arenas active\n3. Check Python shows correct observation/action count\n4. Scale up to 4, then 8 arenas\n\n### Expected Results\n| Arenas | Agents | Expected Speed |\n|--------|--------|----------------|\n| 1 | 2 | ~15 it/s |\n| 2 | 4 | ~30 it/s |\n| 4 | 8 | ~60 it/s |\n| 8 | 16 | ~120 it/s |\n\nNote: Speedup may not be perfectly linear due to physics overhead.\n\n### Python Side\nNo changes needed\\! godot_rl automatically handles multiple AIController3D nodes. The Sync node batches all observations and distributes all actions.\n\n## Checklist\n- [ ] Create multi_training.tscn with Sync node\n- [ ] Add multiple arena instances with proper spacing\n- [ ] Verify each arena resets independently\n- [ ] Verify enemy references are arena-local\n- [ ] Test with 2 arenas, then scale up\n- [ ] Add camera that shows all arenas (for viz mode)\n- [ ] Update README with multi-arena instructions\n\n## Dependencies\n- Existing training_arena.tscn should work as-is when instanced\n- May need to adjust arena_manager.gd if it has global references\n","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T20:28:24.410587-06:00","updated_at":"2025-12-06T20:49:29.724244-06:00","closed_at":"2025-12-06T20:49:29.724244-06:00"}
{"id":"SumoArena-cua","title":"EPIC: Combat Overhaul - Exciting Battles","description":"","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-06T20:46:06.098649-06:00","updated_at":"2025-12-07T12:51:33.989441-06:00","closed_at":"2025-12-07T12:51:33.989441-06:00"}
{"id":"SumoArena-h8g","title":"Cleanup: Migrate old sumo- prefixed beads","description":"The project was copied from another location with 34 old sumo-* prefixed issues. These need to be either:\n1. Deleted (if already duplicated in SumoArena-* prefix)\n2. Migrated (if still relevant and not duplicated)\n\n## Old sumo-* issues still OPEN:\n- sumo-0v1: Replace capsules with sumo blob models (P2 polish)\n- sumo-1q8: Record demo video (P3 polish)\n- sumo-3ay: Run headless training 500k+ steps\n- sumo-46s: Add sound effects (P3 polish)\n- sumo-4s1: EPIC Phase 3\n- sumo-7ws: EPIC Phase 1\n- sumo-9zx: VALIDATE Phase 3\n- sumo-i3l: Add particle effects (P3 polish) - DONE via combat overhaul\n- sumo-pi6: Create evaluation script\n- sumo-zw7: EPIC Phase 4\n- sumo-zzd: EPIC Phase 2\n\n## Analysis:\n- Phase 1 \u0026 2 EPICs: Already closed in SumoArena-* (completed)\n- sumo-i3l (particles): Already implemented in combat overhaul\n- Other polish tasks: May still be relevant\n\n## Fix approach:\nRun: bd import -i .beads/issues.jsonl --rename-on-import\nThis will convert sumo-* to SumoArena-* prefix and merge them properly.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-06T21:08:53.592594-06:00","updated_at":"2025-12-06T21:22:10.958135-06:00","closed_at":"2025-12-06T21:22:10.958135-06:00"}
{"id":"SumoArena-i3l","title":"Add particle effects (collision dust, fall splash)","description":"# Add Particle Effects\n\n## What\nAdd visual feedback particles for collisions and falls.\nEnhances readability of gameplay for observers.\n\n## Effects to Add\n\n### 1. Collision Dust\n- **When**: Agents collide with each other\n- **Appearance**: Small dust puff at contact point\n- **Color**: Light brown/tan\n- **Duration**: 0.3 seconds\n\n### 2. Fall Splash\n- **When**: Agent falls below platform\n- **Appearance**: Circular splash/poof at edge where fell\n- **Color**: Match agent color (blue/red)\n- **Duration**: 0.5 seconds\n\n### 3. Impact Stars (Optional)\n- **When**: High-speed collision\n- **Appearance**: Cartoon impact stars\n- **Color**: White/yellow\n- **Duration**: 0.2 seconds\n\n## Implementation\n\n### Collision Dust\n```gdscript\n# In sumo_agent.gd\n\n@onready var dust_particles: GPUParticles3D = $DustParticles\n\nfunc _physics_process(delta: float) -\u003e void:\n    # ... movement code ...\n    move_and_slide()\n    \n    for i in get_slide_collision_count():\n        var collision = get_slide_collision(i)\n        var collider = collision.get_collider()\n        \n        if collider is CharacterBody3D:\n            # Spawn dust at collision point\n            var contact_point = collision.get_position()\n            spawn_dust(contact_point)\n\nfunc spawn_dust(position: Vector3) -\u003e void:\n    # GPUParticles3D with one-shot\n    dust_particles.global_position = position\n    dust_particles.emitting = true\n```\n\n### Fall Splash\n```gdscript\n# In arena_manager.gd\n\nvar splash_scene = preload(\"res://scenes/splash_effect.tscn\")\n\nfunc _on_agent_fell(agent: CharacterBody3D) -\u003e void:\n    # Spawn splash at edge position (not where agent is now)\n    var fall_pos = agent.global_position\n    fall_pos.y = 0  # Platform level\n    \n    # Clamp to edge of arena\n    var dir_from_center = Vector2(fall_pos.x, fall_pos.z).normalized()\n    fall_pos.x = dir_from_center.x * ARENA_RADIUS\n    fall_pos.z = dir_from_center.y * ARENA_RADIUS\n    \n    var splash = splash_scene.instantiate()\n    splash.global_position = fall_pos\n    splash.modulate = agent.agent_color\n    add_child(splash)\n```\n\n### Particle Node Setup\n```\nGPUParticles3D:\n  amount: 20\n  lifetime: 0.3\n  one_shot: true\n  explosiveness: 0.9\n  ParticleProcessMaterial:\n    emission_shape: SPHERE (radius 0.2)\n    direction: (0, 1, 0) with spread\n    gravity: (0, -5, 0)\n    initial_velocity: 2.0\n    scale: 0.1 → 0.0 over lifetime\n```\n\n## Scene Structure\n```\nCharacterBody3D (sumo_agent.tscn)\n├── MeshInstance3D\n├── CollisionShape3D\n├── AIController3D\n└── DustParticles (GPUParticles3D) ← NEW\n```\n\nSeparate scene for splash:\n```\nsplash_effect.tscn:\nGPUParticles3D (one_shot, auto-queue-free)\n```\n\n## Performance Considerations\n- Use GPUParticles3D (not CPUParticles3D)\n- Limit particle count (20-50 per effect)\n- Short lifetimes\n- One-shot mode (no continuous emission)\n- queue_free() after emission completes\n\n## Testing\n- [ ] Dust appears on collision\n- [ ] Dust appears at contact point (not agent center)\n- [ ] Splash appears when agent falls\n- [ ] Splash is correct color\n- [ ] Performance stays smooth\n- [ ] No memory leaks (particles clean up)\n\n## Acceptance Criteria\n- [ ] Collision dust particles working\n- [ ] Fall splash particles working\n- [ ] Effects enhance visual clarity\n- [ ] No performance impact\n- [ ] Effects are appropriately subtle\n\n## Dependencies\n- Requires: Core gameplay working (Phase 3 complete)\n- Optional: Model replacement done first (looks better with models)","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-06T20:28:24.412285-06:00","updated_at":"2025-12-06T20:28:24.412285-06:00","dependencies":[{"issue_id":"SumoArena-i3l","depends_on_id":"SumoArena-zw7","type":"parent-child","created_at":"2025-12-04T18:31:53.336542-06:00","created_by":"daemon"},{"issue_id":"SumoArena-i3l","depends_on_id":"SumoArena-9zx","type":"blocks","created_at":"2025-12-04T18:31:53.373576-06:00","created_by":"daemon"}]}
{"id":"SumoArena-kp4","title":"Extend arm length by 25%","description":"","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-06T21:53:39.28515-06:00","updated_at":"2025-12-06T21:58:11.450019-06:00","closed_at":"2025-12-06T21:58:11.450019-06:00"}
{"id":"SumoArena-n4r","title":"Fix reset/timer coordination bugs in arena_manager","description":"# Reset/Timer Coordination Bugs\n\n## Symptoms Observed\n- Rewards appear mirrored/same for both agents in some cases\n- Random resets occurring unexpectedly\n- Timer not properly coordinating with win/loss resets\n- Episode step counter possibly not resetting correctly\n\n## Root Cause Analysis Needed\n- Check if `episode_active` flag is being set/cleared at right times\n- Verify `fell_off` signal isn't firing multiple times\n- Check if async `await` in reset is causing race conditions\n- Verify step penalty accumulation vs win/loss reward timing\n\n## Files Involved\n- scripts/arena_manager.gd - episode coordination\n- scripts/sumo_agent.gd - reset() and reward accumulation\n\n## Acceptance Criteria\n- [ ] Winner consistently shows positive reward (~0.7-0.9)\n- [ ] Loser consistently shows negative reward (~-1.1 to -1.3)\n- [ ] No unexpected timeouts during active play\n- [ ] Clean reset cycle without duplicate triggers\n- [ ] Multiple episodes run smoothly","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-06T20:28:24.412803-06:00","updated_at":"2025-12-06T20:28:24.412803-06:00","closed_at":"2025-12-05T12:56:26.965523-06:00"}
{"id":"SumoArena-n8j","title":"Fix swing push direction based on arm side","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T21:53:55.192398-06:00","updated_at":"2025-12-06T21:59:11.032934-06:00","closed_at":"2025-12-06T21:59:11.032934-06:00"}
{"id":"SumoArena-nau","title":"Add impact particle effects","description":"Add GPUParticles3D that burst on collision and swing hits. More particles for harder impacts.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T20:46:36.176197-06:00","updated_at":"2025-12-06T21:03:16.75001-06:00","closed_at":"2025-12-06T21:03:16.75001-06:00"}
{"id":"SumoArena-o53","title":"Add Sync node to training arena","description":"# Add Sync Node to Training Arena\n\n## What\nAdd the godot_rl_agents Sync node to the training_arena scene.\nThe Sync node coordinates communication between Godot and Python.\n\n## Updated Scene Structure\n```\nNode3D (training_arena.tscn root)\n├── Sync ← NEW (from godot_rl_agents plugin)\n├── Platform\n├── Agent1\n│   └── AIController3D\n├── Agent2\n│   └── AIController3D\n├── Camera3D\n└── DirectionalLight3D\n```\n\n## What Sync Node Does\n1. **Collects observations** from all AIController3D nodes\n2. **Sends observations** to Python over socket\n3. **Receives actions** from Python\n4. **Distributes actions** to AIController3D nodes\n5. **Handles episode resets** when any agent's episode ends\n\n## Sync Node Configuration\n\n### Key Properties\n- **Speed Up**: Multiplier for running faster than realtime (training)\n- **Action Repeat**: How many physics frames per action (default: 1)\n- **Handle Physics**: Should sync with _physics_process (usually true)\n\n### Socket Communication\nDefault port: 11008 (can be configured in plugin)\nPython connects to this port to send/receive data.\n\n## Self-Play Configuration\nFor self-play, both agents use the same policy but are separate AIController3D nodes:\n\n```\nSync node automatically:\n1. Finds all AIController3D nodes in scene\n2. Treats each as a separate \"agent\" for RL purposes\n3. Python sees 2 agents, each with 9 obs, 2 actions\n4. Same policy weights are used for both (in Python)\n```\n\n## Arena Manager Integration\nThe arena manager should work with Sync for resets:\n\n```gdscript\n# In arena_manager.gd\n\n@onready var sync_node = $Sync\n\nfunc _ready() -\u003e void:\n    # Connect to sync signals if available\n    if sync_node.has_signal(\"reset_requested\"):\n        sync_node.reset_requested.connect(_on_sync_reset)\n\nfunc request_reset() -\u003e void:\n    \"\"\"Called when episode should end.\"\"\"\n    # Sync node will handle coordinating the reset\n    # across all agents and Python\n    pass\n```\n\n## Testing Without Python\nBefore connecting Python, verify the Sync node is working:\n1. Run scene - should see \"Waiting for connection...\" or similar\n2. No errors in console\n3. Scene still runs (just waiting for Python)\n\n## Testing With Python\nAfter Python training script is ready:\n1. Start Godot scene\n2. Run Python script\n3. Verify connection established\n4. Agents should start taking random actions\n\n## Configuration for Training\n\n### Speed Settings\n```\n# For visualization/debugging\nSpeed Up: 1.0 (realtime)\n\n# For fast training (headless)\nSpeed Up: 10.0 or higher\n```\n\n### Parallel Environments\nFor faster training, run multiple Godot instances:\n- Each instance connects on different port\n- Python's GodotEnv can manage multiple instances\n\n## Acceptance Criteria\n- [ ] Sync node added to training_arena scene\n- [ ] Sync node finds both AIController3D nodes\n- [ ] Scene runs without errors\n- [ ] Console shows waiting for connection\n- [ ] No crashes when Python not connected\n- [ ] Ready for Python connection in Phase 3\n\n## Dependencies\n- Requires: Plugin installed (SumoArena-chj)\n- Requires: AIController3D on agents (SumoArena-486)\n- Requires: Reset logic (SumoArena-6v9)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.413274-06:00","updated_at":"2025-12-06T20:28:24.413274-06:00","closed_at":"2025-12-05T12:13:40.017539-06:00","dependencies":[{"issue_id":"SumoArena-o53","depends_on_id":"SumoArena-zzd","type":"parent-child","created_at":"2025-12-04T18:10:10.112303-06:00","created_by":"daemon"},{"issue_id":"SumoArena-o53","depends_on_id":"SumoArena-6v9","type":"blocks","created_at":"2025-12-04T18:10:10.148681-06:00","created_by":"daemon"}]}
{"id":"SumoArena-p2t","title":"Enhance platform visuals","description":"Add glowing edge ring to platform using torus mesh with emissive material. Makes arena boundary more visible.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-06T20:46:46.759537-06:00","updated_at":"2025-12-06T21:03:27.280688-06:00","closed_at":"2025-12-06T21:03:27.280688-06:00"}
{"id":"SumoArena-pi6","title":"Create evaluation script (eval.py)","description":"# Create Evaluation Script\n\n## What\nCreate an evaluation script to test trained models and compare checkpoints.\nEssential for understanding what the agent learned.\n\n## Evaluation Script Implementation\n\n```python\n# python/eval.py\n\"\"\"\nEvaluate trained Sumo RL models.\n\nFeatures:\n- Load any checkpoint\n- Run evaluation episodes\n- Record videos (optional)\n- Compare multiple checkpoints\n\"\"\"\n\nimport argparse\nimport time\nfrom pathlib import Path\n\nfrom godot_rl.wrappers.stable_baselines_wrapper import StableBaselinesGodotEnv\nfrom stable_baselines3 import PPO\nimport numpy as np\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Evaluate Sumo RL model\")\n    parser.add_argument(\"model_path\", type=str,\n                        help=\"Path to model checkpoint\")\n    parser.add_argument(\"--episodes\", type=int, default=10,\n                        help=\"Number of evaluation episodes\")\n    parser.add_argument(\"--render\", action=\"store_true\",\n                        help=\"Show visualization\")\n    parser.add_argument(\"--slow\", action=\"store_true\",\n                        help=\"Add delays for human viewing\")\n    parser.add_argument(\"--record\", type=str, default=None,\n                        help=\"Directory to save recordings\")\n    return parser.parse_args()\n\ndef main():\n    args = parse_args()\n    \n    print(f\"Loading model from: {args.model_path}\")\n    \n    # Create environment\n    env = StableBaselinesGodotEnv(\n        env_path=None,\n        show_window=True,  # Always show for eval\n        n_parallel=1,\n    )\n    \n    # Load model\n    model = PPO.load(args.model_path, env=env)\n    \n    # Statistics\n    episode_rewards = []\n    episode_lengths = []\n    agent1_wins = 0\n    agent2_wins = 0\n    draws = 0\n    \n    print(f\"\\nRunning {args.episodes} evaluation episodes...\\n\")\n    \n    for ep in range(args.episodes):\n        obs = env.reset()\n        done = False\n        episode_reward = 0\n        steps = 0\n        \n        while not done:\n            action, _ = model.predict(obs, deterministic=True)\n            obs, reward, done, info = env.step(action)\n            episode_reward += reward[0]  # Sum rewards from both agents\n            steps += 1\n            \n            if args.slow:\n                time.sleep(0.016)  # ~60 FPS\n        \n        episode_rewards.append(episode_reward)\n        episode_lengths.append(steps)\n        \n        # Determine winner (based on reward sign)\n        # Positive reward = agent 1 won, negative = agent 2\n        if episode_reward \u003e 0.5:\n            agent1_wins += 1\n            winner = \"Agent 1\"\n        elif episode_reward \u003c -0.5:\n            agent2_wins += 1\n            winner = \"Agent 2\"\n        else:\n            draws += 1\n            winner = \"Draw\"\n        \n        print(f\"Episode {ep + 1}: reward={episode_reward:.2f}, \"\n              f\"steps={steps}, winner={winner}\")\n    \n    # Summary\n    print(\"\\n\" + \"=\"*50)\n    print(\"EVALUATION SUMMARY\")\n    print(\"=\"*50)\n    print(f\"Episodes: {args.episodes}\")\n    print(f\"Mean reward: {np.mean(episode_rewards):.3f} ± {np.std(episode_rewards):.3f}\")\n    print(f\"Mean episode length: {np.mean(episode_lengths):.1f} ± {np.std(episode_lengths):.1f}\")\n    print(f\"Agent 1 wins: {agent1_wins} ({100*agent1_wins/args.episodes:.1f}%)\")\n    print(f\"Agent 2 wins: {agent2_wins} ({100*agent2_wins/args.episodes:.1f}%)\")\n    print(f\"Draws: {draws} ({100*draws/args.episodes:.1f}%)\")\n    \n    env.close()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Usage Examples\n\n### Basic Evaluation\n```bash\npython eval.py runs/20240101_120000/sumo_final --episodes 20\n```\n\n### Slow Motion Viewing\n```bash\npython eval.py runs/latest/sumo_final --episodes 5 --slow\n```\n\n### Compare Checkpoints\n```bash\n# Early checkpoint\npython eval.py runs/run1/checkpoints/sumo_ppo_50000_steps --episodes 10\n\n# Later checkpoint\npython eval.py runs/run1/checkpoints/sumo_ppo_500000_steps --episodes 10\n```\n\n## What to Observe\n\n### Behavioral Analysis\nWatch for:\n- **Approach strategy**: Direct charge vs circling\n- **Edge awareness**: Does agent avoid edges?\n- **Opponent exploitation**: Pushes toward edge when opponent is near\n- **Recovery**: Can recover from near-edge positions\n- **Consistency**: Same strategies across episodes\n\n### Quantitative Metrics\n- **Win rate**: Should be ~50% for self-play (symmetric)\n- **Episode length**: Shorter = more decisive wins\n- **Reward variance**: Lower = more consistent performance\n\n## Comparing Training Stages\nCreate comparison script:\n\n```python\n# python/compare_checkpoints.py\nimport subprocess\nimport re\n\ncheckpoints = [\n    \"runs/run1/checkpoints/sumo_ppo_50000_steps\",\n    \"runs/run1/checkpoints/sumo_ppo_100000_steps\",\n    \"runs/run1/checkpoints/sumo_ppo_200000_steps\",\n    \"runs/run1/checkpoints/sumo_ppo_500000_steps\",\n]\n\nfor ckpt in checkpoints:\n    print(f\"\\n--- {ckpt} ---\")\n    subprocess.run([\"python\", \"eval.py\", ckpt, \"--episodes\", \"20\"])\n```\n\n## Recording Videos (Optional Enhancement)\nFor demos, add video recording:\n```python\n# Requires additional setup (e.g., using gym wrappers or screen capture)\n# This is optional polish for Phase 4\n```\n\n## Acceptance Criteria\n- [ ] eval.py script runs without errors\n- [ ] Can load and evaluate any checkpoint\n- [ ] Statistics are printed correctly\n- [ ] Can compare different checkpoints\n- [ ] Slow mode works for viewing\n- [ ] Insights gained about trained behavior\n\n## Dependencies\n- Requires: Headless training complete (SumoArena-3ay) - need checkpoints to evaluate","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.413748-06:00","updated_at":"2025-12-07T12:51:33.989793-06:00","closed_at":"2025-12-07T12:51:33.989793-06:00","dependencies":[{"issue_id":"SumoArena-pi6","depends_on_id":"SumoArena-4s1","type":"parent-child","created_at":"2025-12-04T18:30:18.3161-06:00","created_by":"daemon"},{"issue_id":"SumoArena-pi6","depends_on_id":"SumoArena-3ay","type":"blocks","created_at":"2025-12-04T18:30:18.355128-06:00","created_by":"daemon"}]}
{"id":"SumoArena-t1e","title":"Add reward shaping (edge pressure + momentum)","description":"Add edge pressure reward (bonus when enemy closer to edge) and momentum reward (bonus for landing hard hits). Discourages passive circling.","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-06T20:46:30.918776-06:00","updated_at":"2025-12-06T21:03:11.476088-06:00","closed_at":"2025-12-06T21:03:11.476088-06:00"}
{"id":"SumoArena-ubo","title":"Implement get_obs() - observation space","description":"# Implement get_obs() - Observation Space\n\n## What\nImplement the observation function that returns 9 floats to the RL agent.\nThis is called every step to provide the agent's \"view\" of the world.\n\n## Observation Space (9 floats)\n\n| Index | Name                  | Range       | Description |\n|-------|----------------------|-------------|-------------|\n| 0     | angle_to_enemy_sin   | [-1.0, 1.0] | sin(angle to enemy in local space) |\n| 1     | angle_to_enemy_cos   | [-1.0, 1.0] | cos(angle to enemy) |\n| 2     | distance_to_enemy    | [0.0, 1.0]  | Normalized by max arena distance |\n| 3     | distance_to_edge     | [0.0, 1.0]  | 0=at edge, 1=center |\n| 4     | enemy_distance_edge  | [0.0, 1.0]  | How close enemy is to falling |\n| 5     | own_velocity_x       | [-1.0, 1.0] | Local X velocity / max_speed |\n| 6     | own_velocity_z       | [-1.0, 1.0] | Local Z velocity / max_speed |\n| 7     | enemy_velocity_x     | [-1.0, 1.0] | Enemy's X velocity (relative) |\n| 8     | enemy_velocity_z     | [-1.0, 1.0] | Enemy's Z velocity (relative) |\n\n## Implementation\n\n```gdscript\n# In sumo_agent.gd\n\nconst ARENA_RADIUS: float = 6.0\nconst MAX_DISTANCE: float = ARENA_RADIUS * 2.0\n\nvar enemy: CharacterBody3D = null  # Set by arena manager\n\nfunc get_obs() -\u003e Array:\n    var obs = []\n    \n    # --- Angle to enemy (in local space) ---\n    var to_enemy = enemy.global_position - global_position\n    var local_to_enemy = to_enemy * global_transform.basis  # Transform to local\n    var angle = atan2(local_to_enemy.x, -local_to_enemy.z)  # Angle in local space\n    obs.append(sin(angle))  # 0: angle_sin\n    obs.append(cos(angle))  # 1: angle_cos\n    \n    # --- Distance to enemy (normalized) ---\n    var dist_to_enemy = to_enemy.length()\n    obs.append(clamp(dist_to_enemy / MAX_DISTANCE, 0.0, 1.0))  # 2: distance\n    \n    # --- Distance to edge (normalized, 0=edge, 1=center) ---\n    var my_dist_from_center = Vector2(\n        global_position.x, \n        global_position.z\n    ).length()\n    var my_edge_dist = (ARENA_RADIUS - my_dist_from_center) / ARENA_RADIUS\n    obs.append(clamp(my_edge_dist, 0.0, 1.0))  # 3: my edge distance\n    \n    # --- Enemy distance to edge ---\n    var enemy_dist_from_center = Vector2(\n        enemy.global_position.x, \n        enemy.global_position.z\n    ).length()\n    var enemy_edge_dist = (ARENA_RADIUS - enemy_dist_from_center) / ARENA_RADIUS\n    obs.append(clamp(enemy_edge_dist, 0.0, 1.0))  # 4: enemy edge distance\n    \n    # --- Own velocity (local, normalized) ---\n    var local_vel = velocity * global_transform.basis\n    obs.append(clamp(local_vel.x / MAX_SPEED, -1.0, 1.0))  # 5: vel_x\n    obs.append(clamp(local_vel.z / MAX_SPEED, -1.0, 1.0))  # 6: vel_z\n    \n    # --- Enemy velocity (in my local space, normalized) ---\n    var enemy_vel = enemy.velocity * global_transform.basis\n    obs.append(clamp(enemy_vel.x / MAX_SPEED, -1.0, 1.0))  # 7: enemy_vel_x\n    obs.append(clamp(enemy_vel.z / MAX_SPEED, -1.0, 1.0))  # 8: enemy_vel_z\n    \n    return obs\n```\n\n## Design Rationale\n\n### Why sin/cos for angle?\nAngles have discontinuity at ±π. When enemy crosses from +179° to -179°, the raw angle jumps from 3.12 to -3.12. Neural networks hate discontinuities.\nsin/cos representation:\n- Continuous everywhere\n- Enemy directly ahead: (sin=0, cos=1)\n- Enemy to right: (sin=1, cos=0)\n- Enemy behind: (sin=0, cos=-1)\n\n### Why normalize everything?\nNeural networks work best with inputs in [-1, 1] or [0, 1].\nNormalization also makes hyperparameters more portable.\n\n### Why include enemy's distance to edge?\nThis is \"tactical information\" - lets the agent:\n- Know when enemy is vulnerable\n- Plan pushes toward the closer edge\n- Recognize opportunities\n\n### Why relative velocities?\nVelocities in local space tell the agent:\n- Am I moving toward/away from enemy?\n- Is enemy moving toward/away from me?\n- Essential for timing collisions and bracing for impact\n\n## Edge Cases\n1. **Enemy is self**: During reset or initialization, enemy might not be set. Check for null.\n2. **At exact center**: distance_to_edge = 1.0 (max), not infinity\n3. **Velocity near zero**: Still normalizes correctly, just near 0\n\n## Testing\nBefore RL training, verify observations look correct:\n```gdscript\nfunc _physics_process(delta: float) -\u003e void:\n    # Debug print observations\n    if Input.is_action_just_pressed(\"ui_accept\"):\n        print(\"Obs: \", get_obs())\n```\n\n## Acceptance Criteria\n- [ ] get_obs() returns Array of exactly 9 floats\n- [ ] All values are properly normalized to expected ranges\n- [ ] Observations update correctly each frame\n- [ ] No errors when enemy is valid\n- [ ] Handles edge cases gracefully\n\n## Dependencies\n- Requires: AIController3D added (SumoArena-486)\n- Requires: Movement script with velocity (SumoArena-205)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.41457-06:00","updated_at":"2025-12-06T20:28:24.41457-06:00","closed_at":"2025-12-05T11:28:17.561503-06:00","dependencies":[{"issue_id":"SumoArena-ubo","depends_on_id":"SumoArena-zzd","type":"parent-child","created_at":"2025-12-04T18:01:41.955899-06:00","created_by":"daemon"},{"issue_id":"SumoArena-ubo","depends_on_id":"SumoArena-486","type":"blocks","created_at":"2025-12-04T18:01:41.990367-06:00","created_by":"daemon"}]}
{"id":"SumoArena-ulj","title":"Set up Python training environment","description":"# Set Up Python Training Environment\n\n## What\nCreate the Python environment and install dependencies for training.\nThis includes godot-rl, stable-baselines3, and supporting packages.\n\n## Python Environment Setup\n\n### Option A: Using venv (Simple)\n```bash\ncd sumo-rl/python\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -r requirements.txt\n```\n\n### Option B: Using conda\n```bash\nconda create -n sumo-rl python=3.10\nconda activate sumo-rl\npip install -r requirements.txt\n```\n\n## Requirements File\nCreate `python/requirements.txt`:\n```\ngodot-rl\u003e=0.4.0\nstable-baselines3\u003e=2.0.0\ntensorboard\u003e=2.10.0\nnumpy\u003e=1.21.0\ntorch\u003e=2.0.0\n```\n\n## Package Explanations\n\n### godot-rl\n- Provides GodotEnv wrapper\n- Handles socket communication with Godot\n- Provides StableBaselinesGodotEnv for SB3 integration\n\n### stable-baselines3\n- Industry-standard RL library\n- PPO implementation (our chosen algorithm)\n- Good logging and saving capabilities\n\n### tensorboard\n- Training visualization\n- Loss curves, reward tracking\n- Episode length monitoring\n\n### torch (PyTorch)\n- Neural network backend for SB3\n- GPU acceleration if available\n- Required by stable-baselines3\n\n## Verify Installation\n```python\n# test_install.py\nfrom godot_rl.wrappers.stable_baselines_wrapper import StableBaselinesGodotEnv\nfrom stable_baselines3 import PPO\nimport torch\n\nprint(\"godot-rl: OK\")\nprint(\"SB3: OK\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n```\n\n## GPU Considerations\n- Training works on CPU but is slower\n- If GPU available, torch will use it automatically\n- For CUDA, ensure compatible CUDA toolkit installed\n- Check with: `python -c \"import torch; print(torch.cuda.is_available())\"`\n\n## Directory Structure After Setup\n```\nsumo-rl/\n├── python/\n│   ├── venv/              # Virtual environment\n│   ├── requirements.txt   # Dependencies\n│   ├── train.py          # Training script (next task)\n│   └── test_install.py   # Verification script\n```\n\n## Acceptance Criteria\n- [ ] Virtual environment created\n- [ ] All packages installed successfully\n- [ ] `import godot_rl` works\n- [ ] `import stable_baselines3` works\n- [ ] Test script runs without errors\n- [ ] GPU status known (available or not)\n\n## Dependencies\n- Requires: Phase 2 Validation complete (SumoArena-590)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.415114-06:00","updated_at":"2025-12-06T20:28:24.415114-06:00","closed_at":"2025-12-05T13:10:54.890867-06:00","dependencies":[{"issue_id":"SumoArena-ulj","depends_on_id":"SumoArena-4s1","type":"parent-child","created_at":"2025-12-04T18:15:33.252467-06:00","created_by":"daemon"},{"issue_id":"SumoArena-ulj","depends_on_id":"SumoArena-590","type":"blocks","created_at":"2025-12-04T18:15:33.28985-06:00","created_by":"daemon"}]}
{"id":"SumoArena-vl6","title":"Buff charge: 2x speed, 7x push multiplier","description":"","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-06T21:53:44.590755-06:00","updated_at":"2025-12-06T21:58:11.451378-06:00","closed_at":"2025-12-06T21:58:11.451378-06:00"}
{"id":"SumoArena-vv3","title":"Implement get_reward() - reward function","description":"# Implement get_reward() - Reward Function\n\n## What\nImplement the reward function that provides learning signal to the RL agent.\nRewards are computed per step and on episode termination.\n\n## Reward Structure\n\n| Event                | Reward  | Frequency     |\n|---------------------|---------|---------------|\n| Opponent falls off  | +1.0    | Once/episode  |\n| You fall off        | -1.0    | Once/episode  |\n| Per step (alive)    | -0.001  | Every step    |\n\n## Implementation\n\n```gdscript\n# In sumo_agent.gd\n\nvar accumulated_reward: float = 0.0\nvar episode_ended: bool = false\n\nfunc get_reward() -\u003e float:\n    \"\"\"\n    Called by AIController3D each step.\n    Returns reward since last call, then resets accumulator.\n    \"\"\"\n    var reward = accumulated_reward\n    accumulated_reward = 0.0\n    return reward\n\nfunc _physics_process(delta: float) -\u003e void:\n    if not episode_ended:\n        # Step penalty - encourages faster resolution\n        accumulated_reward -= 0.001\n    \n    # ... rest of movement code\n\nfunc on_won() -\u003e void:\n    \"\"\"Called by arena manager when opponent falls.\"\"\"\n    accumulated_reward += 1.0\n    episode_ended = true\n\nfunc on_lost() -\u003e void:\n    \"\"\"Called by arena manager when we fall.\"\"\"\n    accumulated_reward -= 1.0\n    episode_ended = true\n\nfunc on_draw() -\u003e void:\n    \"\"\"Called by arena manager on timeout.\"\"\"\n    # No additional reward - just the step penalties\n    episode_ended = true\n```\n\n## Arena Manager Integration\nThe arena manager dispatches win/loss events:\n\n```gdscript\n# In arena_manager.gd\n\nfunc _on_agent1_fell() -\u003e void:\n    agent1.on_lost()\n    agent2.on_won()\n    end_episode()\n\nfunc _on_agent2_fell() -\u003e void:\n    agent2.on_lost()\n    agent1.on_won()\n    end_episode()\n\nfunc _on_timeout() -\u003e void:\n    agent1.on_draw()\n    agent2.on_draw()\n    end_episode()\n```\n\n## Reward Design Rationale\n\n### Why +1/-1 for win/loss?\n- Clear, sparse signal for the primary objective\n- Zero-sum: what one gains, other loses\n- Magnitude 1.0 is conventional, easy to reason about\n\n### Why -0.001 per step?\n- Prevents infinite stalling (agents just avoiding each other)\n- Encourages engagement and faster matches\n- Small enough to not dominate win/loss signal\n- 1000 steps × -0.001 = -1.0 (matches loss penalty)\n\n### Why no distance shaping?\nTempting to add:\n```gdscript\n# DON'T DO THIS (usually)\naccumulated_reward += 0.001 * (1.0 - distance_to_enemy)\n```\nProblems:\n- Agent might just stay close without pushing\n- Can learn to oscillate near enemy for free reward\n- Obscures the true objective\n\nLet the agent discover that approaching leads to pushing leads to winning.\n\n### Why no center-camping reward?\nRewarding staying in center would encourage:\n- Passive defensive play\n- Waiting for opponent to attack first\n- Boring to watch, slow to learn aggression\n\n## Handling Edge Cases\n\n### Both agents fall simultaneously\nVery rare, but possible in edge collision:\n```gdscript\n# In arena_manager.gd\nvar agent1_alive: bool = true\nvar agent2_alive: bool = true\n\nfunc _on_agent1_fell() -\u003e void:\n    agent1_alive = false\n    check_episode_end()\n\nfunc _on_agent2_fell() -\u003e void:\n    agent2_alive = false\n    check_episode_end()\n\nfunc check_episode_end() -\u003e void:\n    if not agent1_alive and not agent2_alive:\n        # Both fell - draw\n        agent1.on_draw()\n        agent2.on_draw()\n    elif not agent1_alive:\n        agent1.on_lost()\n        agent2.on_won()\n    elif not agent2_alive:\n        agent2.on_lost()\n        agent1.on_won()\n    \n    if not agent1_alive or not agent2_alive:\n        end_episode()\n```\n\n### Reward not being consumed\nEnsure reward is read and reset each step:\n```gdscript\nfunc get_reward() -\u003e float:\n    var r = accumulated_reward\n    accumulated_reward = 0.0  # Reset after reading!\n    return r\n```\n\n## Testing\nBefore full training:\n1. Have agent fall off manually\n2. Verify reward is -1.0 for faller, +1.0 for winner\n3. Run for 100 steps, verify accumulated step penalty\n\n## Acceptance Criteria\n- [ ] get_reward() returns accumulated reward since last call\n- [ ] Win gives +1.0, loss gives -1.0\n- [ ] Step penalty of -0.001 accumulates\n- [ ] Rewards reset properly after being read\n- [ ] Arena manager correctly dispatches win/loss/draw\n- [ ] Both agents receive appropriate rewards\n\n## Dependencies\n- Requires: AIController3D added (SumoArena-486)\n- Requires: Fall detection (SumoArena-4ip) - triggers win/loss","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.415633-06:00","updated_at":"2025-12-06T20:28:24.415633-06:00","closed_at":"2025-12-05T11:28:17.562011-06:00","dependencies":[{"issue_id":"SumoArena-vv3","depends_on_id":"SumoArena-zzd","type":"parent-child","created_at":"2025-12-04T18:09:06.517754-06:00","created_by":"daemon"},{"issue_id":"SumoArena-vv3","depends_on_id":"SumoArena-486","type":"blocks","created_at":"2025-12-04T18:09:06.552246-06:00","created_by":"daemon"}]}
{"id":"SumoArena-vvy","title":"Test training with visualization","description":"# Test Training with Visualization\n\n## What\nRun short training session with visualization enabled.\nThis validates the entire pipeline before long headless runs.\n\n## Test Procedure\n\n### Step 1: Start Godot\n```bash\n# In one terminal\ncd /path/to/sumo-rl\ngodot\n# Then run the training_arena scene (F5 or play button)\n```\n\n### Step 2: Start Training\n```bash\n# In another terminal\ncd /path/to/sumo-rl/python\nsource venv/bin/activate\npython train.py --timesteps 10000 --viz\n```\n\n### Step 3: Observe\nWatch both windows:\n- **Godot window**: Agents should start moving\n- **Python terminal**: Should show training progress\n\n## What to Look For\n\n### Connection Phase\n- [ ] \"Connecting to Godot...\" message appears\n- [ ] Connection established without timeout\n- [ ] obs_space and act_space printed correctly\n\n### Early Training (0-5k steps)\n- [ ] Agents take random-looking actions\n- [ ] Episodes end (agents fall off or timeout)\n- [ ] Resets happen correctly\n- [ ] No errors or crashes\n\n### Mid Training (5k-10k steps)\n- [ ] Loss values are printed\n- [ ] Episode rewards are tracked\n- [ ] Possibly seeing slightly less random behavior\n\n### After Training\n- [ ] \"Final model saved\" message\n- [ ] Checkpoint files exist in runs/*/checkpoints/\n- [ ] TensorBoard logs exist in runs/*/tensorboard/\n\n## TensorBoard Monitoring\nWhile training:\n```bash\ntensorboard --logdir runs/\n```\nThen open http://localhost:6006\n\nLook for:\n- **ep_reward_mean**: Should trend upward over time\n- **ep_len_mean**: Episode length\n- **entropy_loss**: Should decrease (less random actions)\n- **value_loss** and **policy_loss**: Should decrease\n\n## Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| Connection timeout | Make sure Godot scene is running first |\n| \"No agents found\" | Check AIController3D nodes exist |\n| Agents don't move | Check get_action() is implemented |\n| Always same behavior | Check random seed, ensure actions are applied |\n| Episode never ends | Check fall detection, timeout logic |\n\n## Success Metrics for 10k Steps\nAt this point, behavior should be:\n- Mostly random exploration\n- Occasional movement toward opponent\n- Not expected: consistent strategy yet\n\n## Acceptance Criteria\n- [ ] Training runs for 10k steps without crash\n- [ ] Checkpoint saved successfully\n- [ ] TensorBoard shows data\n- [ ] Agents visibly take actions\n- [ ] Episodes reset correctly\n- [ ] Ready for longer training run\n\n## Dependencies\n- Requires: Training script (SumoArena-70y)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.416116-06:00","updated_at":"2025-12-06T20:28:24.416116-06:00","closed_at":"2025-12-05T13:39:11.673153-06:00","dependencies":[{"issue_id":"SumoArena-vvy","depends_on_id":"SumoArena-4s1","type":"parent-child","created_at":"2025-12-04T18:20:51.991881-06:00","created_by":"daemon"},{"issue_id":"SumoArena-vvy","depends_on_id":"SumoArena-70y","type":"blocks","created_at":"2025-12-04T18:20:52.027535-06:00","created_by":"daemon"}]}
{"id":"SumoArena-xuk","title":"Create project directory structure","description":"# Create Project Directory Structure\n\n## What\nSet up the folder structure for the Godot project as specified in PLAN_OUTLINE.md:\n\n```\nsumo-rl/\n├── addons/\n│   └── godot_rl_agents/          # Will be added in Phase 2\n├── scenes/\n│   ├── training_arena.tscn       # Main scene (created later)\n│   ├── sumo_agent.tscn           # Agent scene (created later)\n│   └── platform.tscn             # Platform scene (created later)\n├── scripts/\n│   ├── sumo_agent.gd             # Agent logic (created later)\n│   ├── ai_controller.gd          # RL interface (Phase 2)\n│   └── arena_manager.gd          # Episode management (Phase 2)\n├── models/                        # For 3D assets (Phase 4)\n└── python/\n    └── train.py                   # Training script (Phase 3)\n```\n\n## Why\nHaving a clean, organized structure from the start:\n- Makes the project easier to navigate\n- Follows Godot conventions (scenes/, scripts/)\n- Separates concerns (Python training vs Godot game)\n- Prepares for plugin installation (addons/)\n\n## Acceptance Criteria\n- [ ] All directories created\n- [ ] project.godot properly configured (if not already)\n- [ ] Can open project in Godot editor without errors\n\n## Notes\n- The project.godot file already exists (from git status)\n- We may need to configure project settings (physics, display)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.416571-06:00","updated_at":"2025-12-06T20:28:24.416571-06:00","closed_at":"2025-12-04T18:45:02.299036-06:00","dependencies":[{"issue_id":"SumoArena-xuk","depends_on_id":"SumoArena-7ws","type":"parent-child","created_at":"2025-12-04T17:57:15.922567-06:00","created_by":"daemon"}]}
{"id":"SumoArena-xyc","title":"Create training arena scene (training_arena.tscn)","description":"# Create Training Arena Scene\n\n## What\nCreate the main training scene: `scenes/training_arena.tscn`\nThis is where agents fight. Instances the platform and two agents.\n\n## Scene Structure\n```\nNode3D (training_arena.tscn root)\n├── Platform (instance of platform.tscn)\n├── Agent1 (instance of sumo_agent.tscn)\n│   └── position: Vector3(3.0, 0.5, 0)   # Spawn on +X side\n│   └── rotation: Vector3(0, -PI/2, 0)   # Face center (-X direction)\n├── Agent2 (instance of sumo_agent.tscn)\n│   └── position: Vector3(-3.0, 0.5, 0)  # Spawn on -X side\n│   └── rotation: Vector3(0, PI/2, 0)    # Face center (+X direction)\n├── Camera3D\n│   └── Overhead or angled view of arena\n├── DirectionalLight3D\n│   └── Sun-like lighting for visibility\n└── [Later: Sync node - added in Phase 2]\n```\n\n## Agent Spawn Positions\nAgents spawn at radius 3.0 from center (halfway to edge):\n- Close enough to engage quickly\n- Far enough to allow approach strategies\n- Facing each other for immediate confrontation\n\n## Camera Setup\nTwo options to consider:\n\n### Option A: Fixed Overhead (Simpler)\n```\nposition: Vector3(0, 15, 0)\nrotation: Vector3(-PI/2, 0, 0)  # Looking straight down\n```\nPros: Easy to implement, clear view of everything\nCons: Harder to see agent \"facing\" direction\n\n### Option B: Angled View (More Readable)\n```\nposition: Vector3(0, 12, 10)\nrotation: Vector3(-0.8, 0, 0)  # Angled down\n```\nPros: More intuitive, can see agent orientation\nCons: One side of arena is \"closer\"\n\nRecommendation: Start with Option A, switch if needed.\n\n## Lighting\nSimple directional light:\n- Angle: 45° from above\n- Casts shadows (optional for performance)\n- Neutral color (white/warm white)\n\n## Environment (Optional)\n- WorldEnvironment node for ambient lighting\n- Simple sky or solid color background\n- Can enhance later in Phase 4\n\n## Agent Differentiation\nNeed to visually distinguish the two agents:\n- Different colors (blue vs red)\n- Set via exported variable or script in arena\n\n## Input Separation\nFor two-player testing, arena script should configure:\n- Agent1: Uses WASD\n- Agent2: Uses Arrow keys\n\nThis could be done via exported enum or script logic.\n\n## Acceptance Criteria\n- [ ] Scene exists at scenes/training_arena.tscn\n- [ ] Platform instanced and visible\n- [ ] Two agents instanced at spawn positions\n- [ ] Agents face each other\n- [ ] Camera provides clear view of action\n- [ ] Lighting makes scene visible\n- [ ] Agents are visually distinguishable\n\n## Dependencies\n- Requires: Platform scene (SumoArena-8ay)\n- Requires: Agent scene (SumoArena-5j6)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-06T20:28:24.41732-06:00","updated_at":"2025-12-06T20:28:24.41732-06:00","closed_at":"2025-12-04T19:33:22.569418-06:00","dependencies":[{"issue_id":"SumoArena-xyc","depends_on_id":"SumoArena-7ws","type":"parent-child","created_at":"2025-12-04T17:58:56.527562-06:00","created_by":"daemon"},{"issue_id":"SumoArena-xyc","depends_on_id":"SumoArena-8ay","type":"blocks","created_at":"2025-12-04T17:58:56.563576-06:00","created_by":"daemon"},{"issue_id":"SumoArena-xyc","depends_on_id":"SumoArena-5j6","type":"blocks","created_at":"2025-12-04T17:58:56.599592-06:00","created_by":"daemon"}]}
{"id":"SumoArena-ze6","title":"Test and validate combat overhaul","description":"Manual keyboard testing of swing attacks, then run training to verify agents learn aggressive behavior instead of passive circling.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-06T20:46:52.062757-06:00","updated_at":"2025-12-07T12:51:33.988219-06:00","closed_at":"2025-12-07T12:51:33.988219-06:00","dependencies":[{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-1pm","type":"blocks","created_at":"2025-12-06T20:47:59.454759-06:00","created_by":"tombauer"},{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-t1e","type":"blocks","created_at":"2025-12-06T20:48:04.692578-06:00","created_by":"tombauer"},{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-nau","type":"blocks","created_at":"2025-12-06T20:48:09.929652-06:00","created_by":"tombauer"},{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-bao","type":"blocks","created_at":"2025-12-06T20:48:15.166014-06:00","created_by":"tombauer"},{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-p2t","type":"blocks","created_at":"2025-12-06T20:48:20.389796-06:00","created_by":"tombauer"},{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-kp4","type":"blocks","created_at":"2025-12-06T21:54:20.556486-06:00","created_by":"tombauer"},{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-vl6","type":"blocks","created_at":"2025-12-06T21:54:25.809995-06:00","created_by":"tombauer"},{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-09y","type":"blocks","created_at":"2025-12-06T21:54:31.05686-06:00","created_by":"tombauer"},{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-n8j","type":"blocks","created_at":"2025-12-06T21:54:36.311117-06:00","created_by":"tombauer"},{"issue_id":"SumoArena-ze6","depends_on_id":"SumoArena-c7m","type":"blocks","created_at":"2025-12-06T21:54:41.558913-06:00","created_by":"tombauer"}]}
{"id":"SumoArena-zw7","title":"EPIC: Phase 4 - Polish (Optional)","description":"# Phase 4: Polish (Optional)\n\n## Overview\nVisual and audio polish to make the project presentable. This is explicitly optional - the core RL learning is the primary goal. However, polish makes for better demos and can help debug by making agent behavior more readable.\n\n## Visual Improvements\n- **Sumo blob model**: Replace capsules with low-poly sumo characters\n- **Arena texture**: Add ring markings, edge highlighting\n- **Camera work**: Smooth follow cam or dramatic angles\n\n## Effects\n- **Dust on collision**: Particles when agents collide (indicates force)\n- **Splash on fall**: Satisfying visual feedback for ring-out\n- **Edge glow**: Warning when agent is near the edge\n\n## Audio\n- **Impact sounds**: Satisfying thud on collision\n- **Fall sound**: Splash or thud when falling off\n- **Crowd/ambient**: Optional background atmosphere\n\n## Demo Recording\n- Capture training progression at milestones\n- Final video showing evolved strategies\n- Side-by-side early vs late training\n\n## Why Optional\nThis phase doesn't affect RL performance at all. It's purely for presentation. If time is limited, skip this entirely. A working RL agent is more valuable than a pretty one that doesn't learn.\n\n## Dependencies\nRequires Phase 3 (Training) - should have trained agents before polishing.","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-06T20:28:24.417764-06:00","updated_at":"2025-12-06T20:28:24.417764-06:00","dependencies":[{"issue_id":"SumoArena-zw7","depends_on_id":"SumoArena-9zx","type":"blocks","created_at":"2025-12-04T18:33:04.112757-06:00","created_by":"daemon"}]}
{"id":"SumoArena-zzd","title":"EPIC: Phase 2 - RL Integration","description":"# Phase 2: RL Integration\n\n## Overview\nThis epic covers integrating the godot_rl_agents plugin and implementing the full RL interface. By the end, the environment will be ready to receive actions from a Python training script and return observations/rewards.\n\n## Why Self-Play Architecture\nSelf-play is chosen because:\n1. No need to hand-code opponent behavior\n2. Agents learn to counter increasingly skilled opponents\n3. Single policy learns both offense and defense\n4. godot_rl_agents natively supports multi-agent self-play\n\n## Observation Design Rationale\nThe 9-float observation space is intentionally minimal:\n- **angle_to_enemy (sin/cos)**: Using sin/cos avoids the discontinuity problem when the enemy crosses from +180° to -180°. The agent sees smooth transitions.\n- **distance_to_enemy**: Normalized so the agent knows if it can reach the opponent\n- **distance_to_edge**: Critical for survival - agent must know when it's in danger\n- **enemy_distance_to_edge**: Allows exploiting when opponent is vulnerable\n- **velocities**: Enables predicting collisions and planning momentum-based pushes\n\n## Action Design Rationale\nContinuous actions only (no discrete \"shove\" button) because:\n- Simpler action space = faster learning\n- Pure physics-based combat feels more natural\n- Shove mechanic can be added later as an extension\n\n## Reward Shaping\nMinimal reward shaping to avoid unintended behaviors:\n- **+1/-1 for win/loss**: Clear signal for the primary objective\n- **-0.001 per step**: Prevents infinite stalling, encourages engagement\n- **No center-camping bonus**: Would encourage passive play\n- **No distance-to-opponent shaping**: Let the agent discover approaching is useful\n\n## Technical Integration Points\n- AIController3D node attached to each agent\n- Sync node in arena for coordinating episodes\n- get_obs(), get_action(), get_reward() interface implementation\n- Reset logic must handle both agents atomically\n\n## Dependencies\nRequires Phase 1 (Basic Scene) to be complete - we need working physics before adding RL.","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-06T20:28:24.418211-06:00","updated_at":"2025-12-06T20:51:56.49579-06:00","closed_at":"2025-12-06T20:51:56.49579-06:00","dependencies":[{"issue_id":"SumoArena-zzd","depends_on_id":"SumoArena-691","type":"blocks","created_at":"2025-12-04T18:33:04.033103-06:00","created_by":"daemon"}]}
